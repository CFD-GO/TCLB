<?R
	source("conf.R")
	c_header();
?>
/*  File defining Lattice                                      */
/*     Lattice is the low level class defining functionality   */
/*       of Adjoint LBM solver. It realizes all the LBM        */
/*       calculations and data transfer                        */
/*-------------------------------------------------------------*/

#include "Consts.h"
#include "cross.h"
#include "types.h"
#include "Global.h"
#include "Lattice.h"
#include "vtkLattice.h"
#include <mpi.h>
#include <cassert>

#ifdef GRAPHICS
static void MouseMove( Lattice * data, int x, int y, int nx, int ny );
static int MainLoop( uchar4* outputBitmap, Lattice *d, int ticks );
static void MainFree( Lattice *d);
#endif

/// Calculate the Snapshot level for the optimal Checkpointing technique
/**
        C-crazed function for calculating number of zeros at
        the end of a number written in a binary system
        /param i The number
        /return number of zeros at the end of the number written in a binary system
*/
static int SnapLevel(unsigned int i) {
	unsigned int j = 16;
	unsigned int w = 0;
	unsigned int k = 0xFFFFu;
	while(j) {
		if (i & k) {
			j = j >> 1;
			k = k >> j;
		} else {
			w = w + j;
			j = j >> 1;
			k = k << j;
		}
	}
	return w;
}

/// Set position
void Lattice::setPosition(double px_, double py_, double pz_)
{
	px = px_;
	py = py_;
	pz = pz_;
	launcher.container.px = px + 0.5 + getLocalRegion().dx;
	launcher.container.py = py + 0.5 + getLocalRegion().dy;
	launcher.container.pz = pz + 0.5 + getLocalRegion().dz;
}

/// Calculation of the offset from X, Y and Z
int Lattice::Offset(int x, int y, int z)
{
	return x+getLocalRegion().nx*y + getLocalRegion().nx*getLocalRegion().ny*z;
}

static void AllocContainer(CartLatticeContainer& container, int nx, int ny, int nz){
  char * tmp=NULL;
  size_t size;
  size = (size_t) nx*ny*nz*sizeof(flag_t);
  debug2("Allocating: %ld b\n", size);
  CudaMalloc( (void**)&tmp, size );
  debug1("got address: (%p - %p)\n", tmp, (unsigned char*)tmp+size);
  CudaMemset( tmp, 0, size );

  container.nx       = nx;
  container.ny       = ny;
  container.nz       = nz;
  container.NodeType = (flag_t*)tmp;
  container.Q        = nullptr;
}

/// Constructor based on size
/**
        Main constructor of Lattice
        \param _region Local region of the Lattice
        \param ns Number of Snapshots
*/
Lattice::Lattice(CartConnectivity connect, int ns, const UnitEnv& units_)
    : LatticeBase(ZONESETTINGS, ZONE_MAX, units_),
      connectivity(std::move(connect)),
      geometry(std::make_unique<Geometry>(getLocalRegion(), connectivity.global_region, units_)),
      nSnaps(ns)
{
	DEBUG_M;
    AllocContainer(launcher.container, getLocalRegion().nx, getLocalRegion().ny, getLocalRegion().nz);
	sample = std::make_unique<Sampler>(model.get(), units, connectivity.mpi_rank);
	Snaps = std::make_unique<FTabs[]>(nSnaps);
	setPosition(0.0,0.0,0.0);
	DEBUG_M;
	for (int i=0; i < nSnaps; i++) {
		Snaps[i].PreAlloc(getLocalRegion().nx, getLocalRegion().ny, getLocalRegion().nz);
	}
	std::fill_n(iSnaps.get(), maxSnaps, -1);
#ifdef ADJOINT
	aSnaps[0].PreAlloc(getLocalRegion().nx, getLocalRegion().ny, getLocalRegion().nz);
	aSnaps[1].PreAlloc(getLocalRegion().nx, getLocalRegion().ny, getLocalRegion().nz);
#endif
	DEBUG_M;
	MPIInit();
	DEBUG_M;
	CudaAllocFinalize();
	DEBUG_M;
	launcher.container.in = Snaps[0];
	launcher.container.out = Snaps[1];
#ifdef ADJOINT
	launcher.container.adjout = aSnaps[0];
#endif
#ifdef GRAPHICS
    NOTICE("Running graphics at %dx%d\n", getLocalRegion().nx, getLocalRegion().ny);
    bitmap = std::make_unique<GPUAnimBitmap>(getLocalRegion().nx, getLocalRegion().ny, this);
    bitmap->mouse_move( (void (*)(void*,int,int,int,int)) MouseMove);
    bitmap->anim_and_exit( (int (*)(uchar4*,void*,int)) MainLoop, (void (*)(void*))MainFree );
    glutMainLoopEvent();
    debug0("Graphics done");
#endif
}

int Lattice::EventLoop() {
#ifdef GRAPHICS
    bitmap->idle_func();
    glutMainLoopEvent();
#endif
    return 0;
}

/// Preallocation of a buffer (combines allocation into one big allocation)
static void BPreAlloc(void ** ptr, size_t size) {
  CudaMalloc( ptr, size );
}

/// Initialization of MPI buffers
/**
        Initialize all the buffors needed for the MPI data transfer
        \param mpi_ MPI Information (connectivity)
*/
void Lattice::MPIInit()
{
//--------- Initialize MPI buffors
	bufnumber = 0;
#ifndef DIRECT_MEM
	debug2("Allocating MPI buffors ...\n");
	storage_t * ptr = NULL;
	int size, from, to;
	int nx = getLocalRegion().nx, ny=getLocalRegion().ny,  nz=getLocalRegion().nz;
<?R
	for (m in NonEmptyMargin) {
?>
	size = <?R C(m$Size,float=F) ?> * sizeof(storage_t);
	from = connectivity.nodes[connectivity.mpi_rank].<?%s m$opposite_side ?>;
	to = connectivity.nodes[connectivity.mpi_rank].<?%s m$side ?>;
	if ((connectivity.mpi_rank != to) && (size > 0)) {
		CudaMallocHost(&ptr,size);
		mpiout[bufnumber] = ptr;
		gpuout[bufnumber] = NULL;
		nodeout[bufnumber] = to;
		CudaMallocHost(&ptr,size);
		mpiin[bufnumber] = ptr;
		BPreAlloc((void**) & (gpubuf[bufnumber]), size);
		BPreAlloc((void**) & (gpubuf2[bufnumber]), size);
		nodein[bufnumber] = from;
		bufsize[bufnumber] = size;
		bufnumber ++;
	}
<?R
	}
?>
#endif

	debug2("Done (BUFS: %d)\n", bufnumber);
}

/// Starting of unsteady adjoint recording
/**
        Starts tape recording of the iteration process including:
        all the iterations done
        changes of settings
*/
void Lattice::startRecord()
{
	if (reverse_save) {
		ERROR("Nested record! Called startRecord while recording\n");
		exit(-1);
	}
	if (Record_Iter != 0) {
		ERROR("Record tape is not rewound (iter = %d) maybe last adjoint didn't go all the way\n", Record_Iter);
		Record_Iter = 0;
	}
	debug2("Lattice is starting to record (unsteady adjoint)\n");
	if (Snap != 0) {
		warning("Snap = %d at startRecord\n", Snap);
	}
	std::fill_n(iSnaps.get(), maxSnaps, -1);
	{
	    const auto filename = formatAsString("%s_%02d_%02d.dat", snapFileName, D_MPI_RANK, getSnap(0));
		iSnaps[getSnap(0)] = 0;
		save(Snaps[Snap], filename.c_str());
	}
	if (Snap != 0) {
		warning("Snap = %d. Going through disk\n", Snap);
	} else {
		iSnaps[Snap] = 0;
	}
	reverse_save = 1;
	clearAdjoint();
	clearGlobals();
	settings_record.clear();
	settings_i=0;
}

/// Saves solution to binary files
/**
        Dump the primal and adjoint solutions to binary files
        \param filename Prefix/path for the dumped binary files
*/
std::string Lattice::saveSolution(const std::string& filename) {
	auto fn = formatAsString("%s_%d.pri", filename, D_MPI_RANK);
	save(Snaps[Snap], fn.c_str());
#ifdef ADJOINT
	sprintf(fn, "%s_%d.adj", filename, D_MPI_RANK);
	save(aSnaps[aSnap], fn.c_str();
#endif
	return fn;
}

/// Save component/density
/**
	Saves a component/density in a binary file
	\param filename Path/prefix of the file to save
	\param comp Density name to save
*/
int Lattice::saveComp(const std::string& filename, const char* comp) const {
	const int n = getLocalRegion().size();
	const auto fn = formatAsString("%s_%s_%d.comp", filename, comp, D_MPI_RANK);
	auto buf = std::make_unique<real_t[]>(n);
	output("Saving component %s to file %s\n", comp, fn);
    bool somethingWritten = false;
<?R
    for (d in rows(DensityAll)) if (d$parameter) {
?>
	if (strcmp(comp, "<?%s d$name ?>") == 0) {
        lattice->Get_<?%s d$nicename ?>(buf.get();
        somethingWritten = true;
    }
<?R
}
?>
    if (somethingWritten) {
    	FILE * f = fopen(fn.c_str(),"wb");
    	assert(f != NULL);
    	fwrite(buf.get(), sizeof(real_t), n, f);
	    fclose(f);
        output("...saved %s\n", comp);
    } else {
     	output("...not saved %s\n", comp);
    }
	return 0;
}

/// Loads solution for binary files
/**
        Loades the primal and adjoint solutions from binary files
        \param filename Prefix/path for the dumped binary files
*/
void Lattice::loadSolution(const char * filename) {
	char fn[STRING_LEN];
	sprintf(fn, "%s_%d.pri", filename, D_MPI_RANK);
	if (load(Snaps[Snap], fn)) exit(-1);
#ifdef ADJOINT
	sprintf(fn, "%s_%d.adj", filename, D_MPI_RANK);
	load(aSnaps[aSnap], fn);
#endif
}

/// Clear the adjoint solution buffers
/**
        Clear the Adjoint Snapshots
*/
void Lattice::clearAdjoint()
{
#ifdef ADJOINT
	debug1("Clearing adjoint\n");
	aSnaps[0].Clear(getLocalRegion().nx,getLocalRegion().ny,getLocalRegion().nz);
	aSnaps[1].Clear(getLocalRegion().nx,getLocalRegion().ny,getLocalRegion().nz);
#endif
	zSet.ClearGrad();
}

/// Clear the derivatives of the parameters
/**
        Clear the derivative component/density of the adjoint solution
*/
void Lattice::clearDPar()
{ <?R
	for (d in rows(DensityAll)) if ((d$adjoint) && (d$parameter)) { ?>
	Clear_<?%s d$nicename ?>(); <?R
	}
?>
}

/// Finish unsteady adjoint record
/**
        Stops the Adjoint recording process
*/
void Lattice::rewindRecord()
{
	Record_Iter = 0;
	IterateTill(Record_Iter, ITER_NORM);
	debug2("Rewind tape\n");
}

/// Finish unsteady adjoint record
/**
        Stops the Adjoint recording process
*/
void Lattice::stopRecord()
{
	if (Record_Iter != 0) {
		WARNING("Record tape is not rewinded (iter = %d)\n",Record_Iter);
		Record_Iter = 0;
	}
	reverse_save = 0;
	debug2("Stop recording\n");
}

/// Copy GPU to CPU memory
inline void Lattice::MPIStream_A()
{
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], CudaMemcpyDeviceToHost, outStream);
	}
}

/// Copy Buffers between processors
inline void Lattice::MPIStream_B(int tag)
{
        if (bufnumber > 0) {
                DEBUG_M;
                CudaStreamSynchronize(outStream);
                DEBUG_M;
        #ifdef CROSS_MPI_OLD
                MPI_Status status;
                MPI_Request request;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], i+tag, MPMD.local, &request);
                }
                for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
                        MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*connectivity.nodes.size() + connectivity.mpi_rank+bufsize[i], MPMD.local, &status);
                        CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], CudaMemcpyHostToDevice, inStream);
                }
        #else
                MPI_Status status;
                MPI_Request request;
//                MPI_Request recvreq[bufnumber];
                MPI_Request * recvreq = new MPI_Request[bufnumber];
                MPI_Request * sendreq = new MPI_Request[bufnumber];
        //	DEBUG_M;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Irecv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], i+tag, MPMD.local, &recvreq[i]);
                }
        //	DEBUG_M;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], i+tag, MPMD.local, &sendreq[i]);
                }
        //	DEBUG_M;
                #ifdef CROSS_MPI_WAITANY
        //        	DEBUG_M;
                        for (int j = 0; j < bufnumber; j++) {
                                int i;
                                MPI_Waitany(bufnumber, recvreq, &i, MPI_STATUSES_IGNORE);
                                CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], CudaMemcpyHostToDevice, inStream);
                        }
                #else
                        DEBUG_M;
                        MPI_Waitall(bufnumber, recvreq, MPI_STATUSES_IGNORE);
                        DEBUG_M;
                        for (int i = 0; i < bufnumber; i++) {
                                CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], CudaMemcpyHostToDevice, inStream);
                        }
                #endif
		MPI_Waitall(bufnumber, sendreq, MPI_STATUSES_IGNORE);
                delete[] recvreq;
                delete[] sendreq;
        #endif
                DEBUG_M;
                CudaStreamSynchronize(inStream);
                DEBUG_M;
        }
}

void Lattice::SetFirstTabs(int tab0, int tab1) {
	int from, to;
	int i = 0; <?R
	for (m in NonEmptyMargin) { ?>
	from = connectivity.nodes[connectivity.mpi_rank].<?%s m$opposite_side ?>;
	to = connectivity.nodes[connectivity.mpi_rank].<?%s m$side ?>;
	if (connectivity.mpi_rank != to) {
		gpuin[i] = Snaps[tab1].<?%s m$name ?>;
		gpuout[i] = launcher.container.out.<?%s m$name ?> = gpubuf[i];
		nodeout[i] = to;
		nodein[i] = from;
		i ++;
	} else {
		launcher.container.out.<?%s m$name ?> = Snaps[tab1].<?%s m$name ?>;
	} <?R
	} ?>
	launcher.container.in = Snaps[tab0];
}

<?R for (a in rows(Actions)) { ?>
/// Normal (Primal) Iteration
/**
        One Primal Iteration
        \param tab0 Snapshot from which to start
        \param tab1 Snapshot in which to put result
        \param iter_type Type of the iteration
*/
void Lattice::<?%s a$FunName ?>(int tab0, int tab1, int iter_type)
{
	DEBUG_PROF_PUSH("<?%s a$name ?>");
	real_t * tmp;
	int size, from, to;
	int i=0;
	debug1("Iteration %d -> %d type: %d. iter: %d\n", tab0, tab1, iter_type, Iter);
	ZoneIter = (Iter + Record_Iter) % zSet.getLen();

	debug1("ZoneIter: %d (in <?%s a$FunName ?>)\n", ZoneIter);
	data.ZoneIndex = ZoneIter;
	data.MaxZones = zSet.MaxZones;
	SetFirstTabs(tab0, tab1); <?R
	old_stage_level = 0
	action_stages = Stages[a$stages,,drop=FALSE]
	sel = which(action_stages$particle)
	action_stages$first_particle = FALSE
	action_stages$first_particle[head(sel,1)] = TRUE
	action_stages$last_particle = FALSE
	action_stages$last_particle[tail(sel,1)] = TRUE
	for (stage in rows(action_stages)) {
?>
	DEBUG_PROF_PUSH("<?%s stage$name ?>");
<?R if (stage$fixedPoint) { ?> for (int fix=0; fix<100; fix++) { <?R } ?>
<?R if (stage$first_particle) { ?> CopyInParticles(); <?R } ?>
<?R if (old_stage_level > 0) { ?>
	MPIStream_B();
	CudaDeviceSynchronize();
	launcher.container.in = Snaps[tab1]; <?R
    }
    old_stage_level = old_stage_level + 1
?>
	DEBUG_PROF_PUSH("Calculation");
	switch(iter_type & ITER_INTEG){
	case ITER_NO:
		launcher.RunBorder< Primal, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS:
		launcher.RunBorder< Primal, IntegrateGlobals, <?%s stage$name ?> >(kernelStream, data); break;
#ifdef ADJOINT
	case ITER_OBJ:
		launcher.RunBorder< Primal, OnlyObjective, <?%s stage$name ?> >(kernelStream, data); break;
#endif
	}
    CudaStreamSynchronize(kernelStream);
    MPIStream_A();
	switch(iter_type & ITER_INTEG){
	case ITER_NO:
		launcher.RunInterior< Primal, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS:
		launcher.RunInterior< Primal, IntegrateGlobals, <?%s stage$name ?> >(kernelStream, data); break;
#ifdef ADJOINT
	case ITER_OBJ:
		launcher.RunInterior< Primal, OnlyObjective, <?%s stage$name ?> >(kernelStream, data); break;
#endif
	}
	DEBUG_PROF_POP();
<?R if (stage$last_particle) { ?> CopyOutParticles() <?R } ?>
<?R if (stage$fixedPoint) { ?> } // for(fix) <?R } ?>
	DEBUG_PROF_POP();
<?R } ?>
	MPIStream_B();
	CudaDeviceSynchronize();
	Snap = tab1;
	MarkIteration();
	updateAllSamples();
	DEBUG_PROF_POP();
};

/// Adjoint Iteration
/**
        One Adjoint Iteration
        \param tab0 Adjoint Snapshot from which to start
        \param tab1 Adjoint Snapshot in which to put result
        \param iter_type Type of the iteration
*/
inline void Lattice::<?%s a$FunName ?>_Adj(int tab0, int tab1, int adjtab0, int adjtab1, int iter_type)
{
#ifdef ADJOINT
	real_t * tmp;
	int size, from, to;
	int i=0;
	debug1("[%d] Iteration_Adj %d -> %d type: %d\n", D_MPI_RANK, adjtab0, adjtab1, iter_type);
	ZoneIter = (Iter + Record_Iter) % zSet.getLen();
	data.ZoneIndex = ZoneIter;
	data.MaxZones = zSet.MaxZones;

	debug1("ZoneIter: %d (in <?%s a$FunName ?>_Adj)\n", ZoneIter);
<?R
	for (m in NonEmptyMargin) {
?>
	to = connectivity.nodes[connectivity.mpi_rank].<?%s m$opposite_side ?>;
	from = connectivity.nodes[connectivity.mpi_rank].<?%s m$side ?>;
	if (connectivity.mpi_rank != to) {
		gpuout[i] = aSnaps[adjtab0].<?%s m$name ?>;
		gpuin[i] = launcher.container.adjin.<?%s m$name ?> = gpubuf[i];
		nodeout[i] = to;
		nodein[i] = from;
		i ++;
	} else {
		launcher.container.adjin.<?%s m$name ?> = aSnaps[adjtab0].<?%s m$name ?>;
	}
	launcher.container.in.<?%s m$name ?> = Snaps[tab0].<?%s m$name ?>;
	launcher.container.adjout.<?%s m$name ?> = aSnaps[adjtab1].<?%s m$name ?>;
<?R
	}

	for (s in a$stages) {
                 stage = Stages[s,,drop=F] ?>

        MPIStream_A();

	switch(iter_type & ITER_INTEG){
	case ITER_NO:
	        launcher.RunInterior< Adjoint, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS:
	        launcher.RunInterior< Adjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_NO | ITER_STEADY:
	        launcher.RunInterior< SteadyAdjoint, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS | ITER_STEADY:
	        launcher.RunInterior< SteadyAdjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	}

        MPIStream_B();

	DEBUG_M;
	switch(iter_type & ITER_INTEG){
	case ITER_NO:
	        launcher.RunBorder< Adjoint, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS:
	        launcher.RunBorder< Adjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_NO | ITER_STEADY:
	        launcher.RunBorder< SteadyAdjoint, NoGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	case ITER_GLOBS | ITER_STEADY:
	        launcher.RunBorder< SteadyAdjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream, data); break;
	}
	DEBUG_M;
        CudaDeviceSynchronize();
<?R } ?>
	aSnap = adjtab1;
	MarkIteration();
#else
	ERROR("This model doesn't have adjoint!\n");
	exit (-1);
#endif
};

/// Combined Primal+Adjoint Iteration
/**
        One combined Primal+Adjoint Iteration (steepest descent included)
        \param tab0 Snapshot from which to start
        \param tab1 Snapshot in which to put result
        \param adjtab0 Adjoint Snapshot from which to start
        \param adjtab1 Adjoint Snapshot in which to put result
        \param iter_type Type of the iteration
*/
inline void Lattice::<?%s a$FunName ?>_Opt(int tab0, int tab1, int adjtab0, int adjtab1, int iter_type)
{
#ifdef ADJOINT
        <?%s a$FunName ?>(tab0, tab1, iter_type);
        <?%s a$FunName ?>_Adj(tab0, tab1, adjtab0, adjtab1, iter_type | ITER_STEADY);
        launcher.RunInterior< Optimize, NoGlobals, <?%s stage$name ?> > (kernelStream, data);
        launcher.RunBorder< Optimize, NoGlobals, <?%s stage$name ?> > (kernelStream, data);
        CudaDeviceSynchronize();
#else
	ERROR("This model doesn't have adjoint!\n");
	exit (-1);
#endif
};

<?R } ?>


/// Function listing all buffers in FTabs
void Lattice::listTabs(FTabs& tab, int* np, size_t ** size, void *** ptr, size_t * maxsize) {
	int j=0;
	int n;
	int nx = getLocalRegion().nx, ny=getLocalRegion().ny,  nz=getLocalRegion().nz;
	if (maxsize) *maxsize = 0;
	n = <?%d length(NonEmptyMargin) ?>;
	if (np) *np = n;
	if (size) *size = new size_t[n];
	if (ptr) *ptr = new void*[n];
<?R
	for (m in NonEmptyMargin) {
?>
	if (size) (*size)[j] = ((size_t) <?R C(m$Size,float=F) ?>) * sizeof(storage_t);
	if (ptr) (*ptr)[j] = (void*) tab.<?%s m$name ?>;
	if (maxsize) if (size) if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;
<?R
	}
?>
}

size_t Lattice::sizeOfTab() {
<?R
	totsize = 0
	for (m in NonEmptyMargin) totsize = m$Size + totsize
?>
	int nx = getLocalRegion().nx, ny=getLocalRegion().ny,  nz=getLocalRegion().nz;
	return <?R C(totsize,float=F) ?>;
}

void Lattice::saveToTab(real_t * rtab, int snap) {
	char * vtab = (char*)rtab;
	void ** ptr;
	size_t * size;
	int n;
	FTabs tab = Snaps[snap];
	listTabs(tab, &n, &size, &ptr, NULL);
	for(int i=0; i<n; i++)
	{
		debug1("Save buf %d of size %ld\n",i,size[i]);
		CudaMemcpy( vtab, ptr[i], size[i], CudaMemcpyDeviceToHost);
		vtab += size[i];
	}
	delete[] size;
	delete[] ptr;
}

void Lattice::loadFromTab(real_t * rtab, int snap) {
	char * vtab = (char*)rtab;
	void ** ptr;
	size_t * size;
	int n;
	FTabs tab = Snaps[snap];
	listTabs(tab, &n, &size, &ptr, NULL);
	for(int i=0; i<n; i++)
	{
		debug1("Load buf %d of size %ld\n",i,size[i]);
		CudaMemcpy( ptr[i], vtab, size[i], CudaMemcpyHostToDevice);
		vtab += size[i];
	}
	delete[] size;
	delete[] ptr;
}


/// Save a FTabs
int Lattice::save(FTabs& tab, const char * filename) {
	FILE * f = fopen(filename, "w");
	if (f == NULL) {
		ERROR("Cannot open %s for output\n", filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt=NULL;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
        output("Saving data slice %d, size %d", i, size[i]);
		CudaMemcpy( pt, ptr[i], size[i], CudaMemcpyDeviceToHost);
		fwrite(pt, size[i], 1, f);
	}

	CudaFreeHost(pt);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

/// Load a FTabs
int Lattice::load(FTabs& tab, const char * filename) {
	FILE * f = fopen(filename, "r");
	output("Loading Lattice data from %s\n", filename);
	if (f == NULL) {
		ERROR("Cannot open %s for output\n", filename);
		return -1;
	}

	void ** ptr;
	void * pt = NULL;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		int ret = fread(pt, size[i], 1, f);
		if (ret != 1) ERROR("Could not read in Lattice::load");
		CudaMemcpy( ptr[i], pt, size[i], CudaMemcpyHostToDevice);
	}

	CudaFreeHost(pt);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

static void FreeContainer(CartLatticeContainer& container) {
  CudaFree(container.NodeType);
  if (container.Q)
    CudaFree(container.Q);
}

/// Destructor
/**
        I think it doesn't leave a big mess
*/
Lattice::~Lattice()
{
	RFI.Close();
    CudaAllocFreeAll();
	FreeContainer(launcher.container);
	for (int i=0; i<nSnaps; i++) {
		Snaps[i].Free();
	}
}

/// Render Graphics (GUI)
/**
        Renders graphics in the GUI version
*/
void Lattice::Color(uchar4 * ptr) {
	launcher.Color(ptr, data);
}

/// Initialization of lattice nodes
/**
        Initialization of the variables in the lattice nodes
*/
void Lattice::Init()
{
   output("Initializing Lattice ...\n");
	iSnaps[getSnap(0)]=0;
	iSnaps[0]=0;
	Record_Iter=0;
	Iter = 0;
	data.iter = 0;
	Snap=0;
   MPI_Barrier(MPMD.local);
   Action_Init(1,0,ITER_NO);
   MPI_Barrier(MPMD.local);
	Iter = 0;
	data.iter = 0;
	clearGlobals();
}

/// Function for calculating the index of a Snapshot for a iteration
int Lattice::getSnap(int i) {
	int s = SnapLevel(i) + 1;
	return s;
}

/// Iterate Primal till a specific iteration
/**
        Function which reconstructs a state in iteration "it"
        by reading a snapshot and iterating from it.
        Used in usteady adjoint
        \param it Iteration on which to finish
        \param iter_type Type of iterations to run
*/
void Lattice::IterateTill(int it, int iter_type)
{
	int s1,ls1,s2,ls2;
	int mx = -1, imx = -1, i;
	for (i=0;i<maxSnaps; i++) {
		if ((iSnaps[i] > mx) && (iSnaps[i] <= it)) {
			mx = iSnaps[i];
			imx = i;
		}
	}
	assert(imx >= 0);
	debug2("iterate: %d -> %d (%d primal) startSnap: %d\n", mx, it, it-mx, imx);
	if (imx >= nSnaps) {
		if (reverse_save) {
			debug2("Reverse Adjoint Disk Read it:%d level:%d\n", mx, imx);
			const auto filename = formatAsString("%s_%02d_%02d.dat", snapFileName, D_MPI_RANK, imx);
			if (load(Snaps[0], filename.c_str())) exit(-1);
		} else {
			ERROR("I have to read from disk, but reverse_save is not switched on\n");
			exit (-1);
		}
		iSnaps[0] = iSnaps[imx];
		imx = 0;
	}
	s1 = imx;
	Record_Iter = mx;
	Snap = s1;
	for (i = mx; i < it; i++) {
		int s2 = getSnap(i+1);
		int s3 = s2;
		if (s2 >= nSnaps) s3=0;
		pop_settings();
		Iteration(s1, s3, iter_type);
		Record_Iter = i+1;
		if (s2 >= nSnaps){
			if (reverse_save) {
				debug2("Reverse Adjoint Disk Write it:%d level:%d\n", i+1, s2);
				const auto filename = formatAsString("%s_%02d_%02d.dat", snapFileName, D_MPI_RANK, s2);
				save(Snaps[0], filename.c_str());
			}
		}
		iSnaps[s2] = i+1;
		iSnaps[s3] = i+1;
		s1 = s3;
	}
}

/// Main iteration function.
/**
        Makes "niter" iterations of type "iter_type"
        Dispatches specific iteration methods for each type
        takes care of calculation of Globals
        \param niter How many iterations to make
        \param iter_type What type of iterations to make
*/
void Lattice::Iterate(int niter, int iter_type)
{
	int last_glob = iter_type & ITER_LASTGLOB;
	int i;
	InitialIteration(niter);
	if (iter_type & ITER_INTEG) last_glob=0;
	if (reverse_save) {
	        switch (iter_type & ITER_TYPE) {
	        case ITER_ADJOINT:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					data.clearGlobals();
					iter_type = iter_type | ITER_GLOBS;
				}
				Record_Iter -= 1;
				if (Record_Iter < 0) {
					ERROR("Went below the begining of the tape (usteady adjoint) !");
					exit(-1);
				}
				IterateTill(Record_Iter, ITER_NORM);
				Iteration_Adj(Snap, (Snap+1) % 2, aSnap % 2, (aSnap+1) % 2, iter_type);
			}
			break;
                case ITER_NORM:
			if (last_glob) {
				IterateTill(niter+Record_Iter-1, iter_type);
				data.clearGlobals();
				IterateTill(Record_Iter+1, iter_type | ITER_GLOBS);
			} else {
				IterateTill(niter+Record_Iter, iter_type);
			}
			break;
                case ITER_OPT:
                        ERROR("Cannot do a OPT iteration in record mode\n");
                        exit(-1);
                        break;
		}
	} else {
	        switch (iter_type & ITER_TYPE) {
	        case ITER_ADJOINT:
	                iter_type |= ITER_STEADY;
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					data.clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration_Adj(Snap, (Snap+1) % 2, aSnap, (aSnap+1) % 2, iter_type);
			}
			break;
                case ITER_NORM:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					data.clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration(Snap, (Snap+1) % 2, iter_type);
				Iter ++;
				data.iter ++;
			}
			break;
                case ITER_OPT:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					data.clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration_Opt(Snap, (Snap+1) % 2, aSnap, (aSnap+1) % 2, iter_type);
				Iter ++;
				data.iter ++;
			}
                        break;
		}
	}
	if (last_glob) {
	        switch (iter_type & ITER_TYPE) {
                case ITER_OPT:
			clearGlobals();
	        case ITER_ADJOINT:
			clearGlobals_Adj();
			break;
                case ITER_NORM:
			clearGlobals();
			break;
		}
		calcGlobals();
	} else if ( iter_type & ITER_INTEG ) {
		calcGlobals();
	}
	FinalIteration();
};

/// Run Action
void Lattice::IterateAction(int action, int niter, int iter_type)
{
	int last_glob = iter_type & ITER_LASTGLOB;
	int i;
	InitialIteration(niter);
	if (iter_type & ITER_INTEG) last_glob=0;
	if (reverse_save) {
		ERROR("Run Action not supported in Adjoint"); exit(-1);
	} else {
	        switch (iter_type & ITER_TYPE) {
	        case ITER_ADJOINT:
			ERROR("Run Action not supported in Adjoint"); exit(-1);
			break;
                case ITER_NORM:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					data.clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				RunAction(action, Snap, (Snap+1) % 2, iter_type);
				Iter ++;
				data.iter ++;
			}
			break;
                case ITER_OPT:
			ERROR("Run Action not supported in Adjoint"); exit(-1);
                        break;
		}
	}
	if (last_glob) {
	        switch (iter_type & ITER_TYPE) {
                case ITER_OPT:
			clearGlobals();
	        case ITER_ADJOINT:
			clearGlobals_Adj();
			break;
                case ITER_NORM:
			clearGlobals();
			break;
		}
		calcGlobals();
	} else if ( iter_type & ITER_INTEG ) {
		calcGlobals();
	}
	FinalIteration();
};

/// Overwrite NodeType in a region
void Lattice::FlagOverwrite(flag_t * mask, const lbRegion& over)
{
	if (getLocalRegion().isEqual(over)) {
		output("overwriting all flags\n");
		CudaMemcpy(launcher.container.NodeType, mask, sizeof(flag_t)*getLocalRegion().sizeL(), CudaMemcpyHostToDevice);
	} else {
		lbRegion inter = getLocalRegion().intersect(over);
	        int x = inter.dx;
	        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
	        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
	        {
	                CudaMemcpy2D(&launcher.container.NodeType[getLocalRegion().offsetL(x,y,z)], sizeof(flag_t), &mask[over.offsetL(x,y,z)], sizeof(flag_t), sizeof(flag_t), inter.nx, CudaMemcpyHostToDevice);
	        }
	}
}

static void ContainerActivateCuts(CartLatticeContainer& container){
  if (!container.Q) {
    void * tmp;
    size_t size = (size_t) container.nx*container.ny*container.nz*sizeof(cut_t)*26;
    debug2("Allocating: %ld b\n", size);
    CudaMalloc( (void**)&tmp, size );
    debug1("got address: (%p - %p)\n", tmp, (unsigned char*)tmp+size);
    CudaMemset( tmp, 0, size );
    container.Q = (cut_t*)tmp;
  }
}

void Lattice::CutsOverwrite(cut_t * Q, const lbRegion& over)
{
	if (Q == NULL) return;
    ContainerActivateCuts(launcher.container);
	lbRegion inter = getLocalRegion().intersect(over);
        int x = inter.dx;
	size_t regsize = getLocalRegion().size();
	size_t oversize = over.size();
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        for (int d = 0; d<26; d++)
        {
                CudaMemcpy2D(&launcher.container.Q[getLocalRegion().offsetL(x,y,z)+d*regsize], sizeof(cut_t), &Q[over.offsetL(x,y,z)+oversize*d], sizeof(cut_t), sizeof(cut_t), inter.nx, CudaMemcpyHostToDevice);
        }

}

/// Get NodeType's from a region
void Lattice::GetFlags(const lbRegion& over, flag_t * NodeType) const
{
	size_t offset;
	lbRegion inter = getLocalRegion().intersect(over);
	if (getLocalRegion().isEqual(over)) {
                CudaMemcpy(NodeType, launcher.container.NodeType, sizeof(flag_t)*getLocalRegion().sizeL(), CudaMemcpyDeviceToHost);
        } else {
	        int x = inter.dx;
	        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
	        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
	        {
			offset = getLocalRegion().offsetL(x,y,z);
			CudaMemcpy2D(&NodeType[over.offsetL(x,y,z)], sizeof(flag_t), &launcher.container.NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, CudaMemcpyDeviceToHost);
		}
	}
}

void Lattice::GetCoords(real_t* tab) {
	return;
}

void Lattice::Get_Field(int id, real_t * tab) { <?R
	for (f in rows(Fields)) if (f$parameter) { ?>
	if (id == <?%s f$Index ?>) return Get_<?%s f$nicename ?>(tab); <?R
	} ?>
}
void Lattice::Get_Field_Adj(int id, real_t * tab) {
#ifdef ADJOINT <?R
	for (f in rows(Fields)) if (f$parameter) { ?>
	if (id == <?%s f$Index ?>) return Get_<?%s f$nicename ?>_Adj(tab); <?R
	} ?>
#endif
}
void Lattice::Set_Field(int id, real_t * tab) { <?R
	for (f in rows(Fields)) if (f$parameter) { ?>
	if (id == <?%s f$Index ?>) return Set_<?%s f$nicename ?>(tab); <?R
	} ?>
}


<?R
	for (f in rows(Fields)[Fields$parameter]) {

	for (adjoint in c(TRUE,FALSE)) {
	if (adjoint) {
		from="aSnaps[aSnap]";
		suff = "_Adj"
	} else {
		from="Snaps[Snap]";
		suff = ""
	}
	ifdef(adjoint)
?>

/// Get [<?%s f$comment ?>]
/**
        Retrive the values of the density <?%s f$nicename ?> (<?%s f$comment ?>)
        from the GPU memory
*/
void Lattice::Get_<?%s f$nicename ?><?%s suff ?>(real_t * tab)
{
	debug2("Pulling all <?%s f$nicename ?>\n");
	CudaMemcpy(
		tab,
		&<?%s from ?>.block14[<?%s f$Index ?>*getLocalRegion().sizeL()],
		getLocalRegion().sizeL()*sizeof(real_t),
		CudaMemcpyDeviceToHost);
}

/// Clear [<?%s f$comment ?>]
/**
        Clear (set to zero) the values of
        the density <?%s f$nicename ?> (<?%s f$comment ?>)
        in the GPU memory
*/
void Lattice::Clear_<?%s f$nicename ?><?%s suff ?>()
{
	debug2("Clearing all <?%s f$nicename ?>\n");
	CudaMemset(
		&<?%s from ?>.block14[<?%s f$Index ?>*getLocalRegion().sizeL()],
		0,
		getLocalRegion().sizeL()*sizeof(real_t));
}

/// Set [<?%s f$comment ?>]
/**
        Set the values of
        the density <?%s f$nicename ?> (<?%s f$comment ?>)
        in the GPU memory
*/
void Lattice::Set_<?%s f$nicename ?><?%s suff ?>(real_t * tab)
{
	debug2("Setting all <?%s f$nicename ?>\n");
	CudaMemcpy(
		&<?%s from ?>.block14[<?%s f$Index?>*getLocalRegion().sizeL()],
		tab,
		getLocalRegion().sizeL()*sizeof(real_t),
		CudaMemcpyHostToDevice);
}

<?R }
ifdef()
} ?>


/// Get Quantity
/**
        Retrive the values of the Quantity
        from the GPU memory
*/
void Lattice::GetQuantity(int quant, const lbRegion& over, real_t * tab, real_t scale)
{
	switch(quant) {	<?R
		for (q in rows(Quantities)) { ifdef(q$adjoint);
	?>
		case <?%s q$Index ?>: return Get<?%s q$name ?>(over, (<?%s q$type ?> *) tab, scale); <?R
		}
		ifdef();
	?>
	}
}


<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>
/// Get [<?%s q$comment ?>]
/**
        Retrive the values of the Quantity <?%s q$name ?> (<?%s q$comment ?>)
        from the GPU memory
*/
void Lattice::Get<?%s q$name ?>(const lbRegion& over, <?%s q$type ?> * tab, real_t scale)
{
	launcher.container.in = Snaps[Snap];
	<?R if (q$adjoint) { ?> launcher.container.adjin = aSnaps[aSnap]; <?R } ?>

	lbRegion inter = getLocalRegion().intersect(over);
	if (inter.size()==0) return;
	<?%s q$type ?> * buf=NULL;
	CudaMalloc((void**)&buf, inter.sizeL()*sizeof(<?%s q$type ?>));
    {
        lbRegion small = inter;
        small.dx -= getLocalRegion().dx;
        small.dy -= getLocalRegion().dy;
        small.dz -= getLocalRegion().dz;
        launcher.GetQuantity<?%s q$name ?>(small, buf, scale, data);
        CudaMemcpy(tab, buf, small.sizeL()*sizeof(<?%s q$type ?>), CudaMemcpyDeviceToHost);
	}
	CudaFree(buf);
}

<?R }; ifdef() ?>

int Lattice::getPars(ParStruct& par_struct) {
  int j = 0;
  <?R if ("DesignSpace" %in% NodeTypes$name) { ?>
  for (int i = 0; i < getLocalRegion().size(); ++i) {
  	if (geometry->geom[i] & NODE_DesignSpace) {
  		j++;
  	}
  }
  <?R } ?>
  par_struct.Par_size = j * <?%d sum(Density$parameter==T) ?>;
  debug1("Par_size: %d\n",par_struct.Par_size);
  MPI_Gather(&par_struct.Par_size, 1, MPI_INT, par_struct.Par_sizes.get(), 1, MPI_INT, 0, MPMD.local);
  if (connectivity.mpi_rank == 0) {
  	par_struct.Par_disp[0] = 0;
  	const auto mpi_size = connectivity.nodes.size();
  	for (size_t i = 0; i < mpi_size - 1; ++i)
  	    par_struct.Par_disp[i + 1] = par_struct.Par_disp[i] + par_struct.Par_sizes[i];
  	for (size_t i = 0; i < mpi_size; ++i)
  	    debug2("Proc: %d Parameters: %d Disp: %d\n", i, par_struct.Par_sizes[i], par_struct.Par_disp[i]);
  	return par_struct.Par_disp[mpi_size-1] + par_struct.Par_sizes[mpi_size-1];
  }
  return 0;
}

int Lattice::getDPar(const ParStruct& par_struct, double * wb) {
  int n = getLocalRegion().size();
  int k = par_struct.Par_size;
  auto buf = std::make_unique<real_t[]>(n);
  auto wb_l = std::make_unique<double[]>(par_struct.Par_size);
  int j = 0;
  double sum = 0;
#ifdef ADJOINT
<?R for (d in rows(Density)) if ((d$parameter)) { ?>
  Get_<?%s d$nicename ?>_Adj(buf.get());
<?R if ("DesignSpace" %in% NodeTypes$name) { ?>
  for (int i=0; i<n; i++) {
    if (geometry->geom[i] & NODE_DesignSpace) {
      wb_l[j] = buf[i];
      sum += wb_l[j]*wb_l[j];
      j++;
    }
  }
<?R } ?>
<?R } ?>
#endif
  output("L2 norm of gradient: %lg\n", sqrt(sum));
  assert(j == par_struct.Par_size);
  MPI_Gatherv(wb_l.get(), par_struct.Par_size, MPI_DOUBLE, wb, par_struct.Par_sizes.get(), par_struct.Par_disp.get(), MPI_DOUBLE, 0, MPMD.local);
  return 0;
}

int Lattice::getPar(const ParStruct& par_struct, double * wb) {
  int n = getLocalRegion().size();
  int k = par_struct.Par_size;
  auto buf = std::make_unique<real_t[]>(n);
  auto wb_l = std::make_unique<double[]>(par_struct.Par_size);
  int j=0;
<?R for (d in rows(Density)) if (d$parameter) { ?>
  Get_<?%s d$nicename ?>(buf.get());
<?R if ("DesignSpace" %in% NodeTypes$name) { ?>
  for (int i=0; i<n; i++) {
    if (geometry->geom[i] & NODE_DesignSpace) {
      wb_l[j] = buf[i];
      j++;
    }
  }
<?R } ?>
<?R } ?>
  assert(j == par_struct.Par_size);
  MPI_Gatherv(wb_l.get(), par_struct.Par_size, MPI_DOUBLE, wb, par_struct.Par_sizes.get(), par_struct.Par_disp.get(), MPI_DOUBLE, 0, MPMD.local);
  return 0;
}

int Lattice::setPar(const ParStruct& par_struct, double * w) {
  static int en=0;
  en++;
  int n = getLocalRegion().size();
  auto buf = std::make_unique<real_t[]>(n);
  auto w_l = std::make_unique<double[]>(par_struct.Par_size);
  DEBUG_M;
  MPI_Scatterv(w, par_struct.Par_sizes.get(), par_struct.Par_disp.get(),  MPI_DOUBLE, w_l.get(), par_struct.Par_size, MPI_DOUBLE, 0, MPMD.local);
  DEBUG_M;
  int j=0;
  double sum =0;
  double diff;
<?R for (d in rows(Density)) if (d$parameter) { ?>
  DEBUG_M;
  Get_<?%s d$nicename ?>(buf.get());
  DEBUG_M;
  <?R if ("DesignSpace" %in% NodeTypes$name) { ?>
  for (int i=0; i<n; i++) {
    if (geometry->geom[i] & NODE_DesignSpace) {
      diff = buf[i];
      buf[i] = w_l[j];
      assert(w_l[j] <= 1.001);
      diff -= buf[i];
      sum += diff*diff;
      j++;
    }
  }
<?R } ?>
  DEBUG_M;
  Set_<?%s d$nicename ?>(buf.get());
  DEBUG_M;
<?R } ?>
  assert(j == par_struct.Par_size);
  output("[%d] L2 norm of parameter change: %lg\n", sqrt(sum));
  return 0;
}

int Lattice::loadComp(const char* filename, const char* comp) const {
    const int n = getLocalRegion().size();
    const auto fn = formatAsString("%s_%d.comp", filename, D_MPI_RANK);
    auto buf = std::make_unique<real_t[]>(n);
    output("Loading component %s from file %s\n", comp, fn.c_str());
    FILE * f = fopen(fn.c_str(), "rb");
    assert(f != NULL);
    int nn = fread(buf.get(), sizeof(real_t), n, f);
    assert(n == nn);
    fclose(f);
<?R for (d in rows(DensityAll)) if (d$parameter) { ?>
    if (std::string_view(comp) == "<?%s d$name ?>") Set_<?%s d$nicename ?>(buf.get(); <?R
} ?>
    return 0;
}

void Lattice::resetAverage() {
	data.reset_iter = data.iter;
        <?R for (f in rows(Fields))  if (f$average) { ?>
          CudaMemset(&Snaps[Snap].block14[<?%s f$Index ?>*getLocalRegion().sizeL()],0,getLocalRegion().sizeL()*sizeof(real_t));
	 <?R } ?>
}
<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>
void Lattice::GetSample<?%s q$name ?>(const lbRegion& over, real_t scale,real_t* buf)
{
    launcher.container.in = Snaps[Snap];
<?R if (q$adjoint) { ?>
    launcher.container.adjin = aSnaps[aSnap]; <?R } ?>
    lbRegion small = getLocalRegion().intersect(over);
    launcher.SampleQuantity<?%s q$name ?>(small, (<?%s q$type ?>*)buf, scale, data);
}
<?R } ;ifdef() ?>
void Lattice::updateAllSamples(){
 if (sample->size != 0) {
	for (size_t j = 0; j < sample->spoints.size(); j++) {
		if (connectivity.mpi_rank == sample->spoints[j].rank) {
		<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>
	 	 if (sample->quant->in("<?%s q$name ?>"))
		{
                        double v = sample->units->alt("<?%s q$unit ?>");
			GetSample<?%s q$name ?>(sample->spoints[j].location,1/v,&sample->gpu_buffer[sample->location["<?%s q$name ?>"]+(data.iter - sample->startIter)*sample->size + sample->totalIter*j*sample->size]);
		}
		<?R }; ifdef() ?>
	} }
 }
}

/// Preallocation of a FTabs
/**
  Aglomerates all the allocation into one big memory chunk
*/
void FTabs::PreAlloc(int nx,int ny,int nz) {
  size_t size;
<?R for (m in NonEmptyMargin) { ?>
  size = <?R C(m$Size,float=F) ?>*sizeof(storage_t);
  CudaPreAlloc( (void**)&<?%s m$name ?>, size );
<?R } ?>
}

/// Clearing (zero-ing) of a FTabs
void FTabs::Clear(int nx,int ny,int nz) {
  size_t size;
<?R for (m in NonEmptyMargin) { ?>
  size = <?R C(m$Size,float=F) ?>*sizeof(storage_t);
  CudaMemset( <?%s m$name ?>, 0, size );
<?R } ?>
}

static void NullSafeFree(void * ptr) {
  if (ptr)
#ifdef DIRECT_MEM
    CudaFreeHost(ptr);
#else
    CudaFree(ptr);
#endif
}

/// Free FTabs memory
void FTabs::Free() { <?R
for (m in NonEmptyMargin) { ?>
  NullSafeFree(<?%s m$name ?>);
  <?%s m$name ?> = nullptr;<?R
} ?>
}

#ifdef GRAPHICS
/// Mouse Move callback (GUI)
/**
	Function called when mouse is moved, wile button pressed in GUI version
*/
void MouseMove( Lattice* data, int x, int y, int nx, int ny )
{
	lbRegion r(
		x,
		data->getLocalRegion().ny - y - 1,
		0,
	1,1,1);
	<?R if ("Wall" %in% NodeTypes$name) { ?>
		flag_t NodeType = NODE_Wall;
		data->FlagOverwrite(&NodeType,r); // Overwrite mesh flags with flags from 'mask' table
	<?R } ?>
}

/// Refresh callback (GUI)
/**
	Function called when refresh of the window is needed.
	Renders the graphics inside of the window in GUI version
*/
int MainLoop( uchar4* outputBitmap, Lattice* data, int ticks )
{
	data->Color(outputBitmap); // Updating graphics
	return 0;
}

/// Clean-up
/**
	Empty now. TODO
*/
void MainFree(Lattice*) {}

// Graphics global objects
PFNGLBINDBUFFERARBPROC    glBindBuffer     = nullptr;
PFNGLDELETEBUFFERSARBPROC glDeleteBuffers  = nullptr;
PFNGLGENBUFFERSARBPROC    glGenBuffers     = nullptr;
PFNGLBUFFERDATAARBPROC    glBufferData     = nullptr;
#endif
