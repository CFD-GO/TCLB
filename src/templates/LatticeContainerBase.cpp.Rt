<?R
	source("conf.R")
        c_header()
    tminx = min(0,Fields$minx)
    tmaxx = max(0,Fields$maxx)
    tminy = min(0,Fields$miny)
    tmaxy = max(0,Fields$maxy)
    tminz = min(0,Fields$minz)
    tmaxz = max(0,Fields$maxz)
?>

#include "LatticeContainerBase.h"
#include "ArbitraryLattice.h"
#include "../Global.h"
#include "../SyntheticTurbulence.h"
#include "../BallTree.h"
#include "ModelConsts.h"
#include <mpi.h>
#define ALLOCPRINT1 debug2("Allocating: %ld b\n", size)
#define ALLOCPRINT2 debug1("got address: (%p - %p)\n", tmp, (unsigned char*)tmp+size)

#ifdef ENABLE_NVPROF
	#include <nvToolsExt.h>
	#define DEBUG_PROF_PUSH(x__) nvtxRangePushA(x__)
	#define DEBUG_PROF_POP() nvtxRangePop()
#else
	#define DEBUG_PROF_PUSH(x__)
	#define DEBUG_PROF_POP()
#endif

// destructor
/*LatticeContainerBase::~LatticeContainerBase() {

}*/

// ?
int _xsdim = 0;
int _ysdim = 1;

/// Fast modulo for one k
CudaDeviceFunction inline int mymod(int x, int k){
    if (x >= k) return x-k;   
    if (x < 0) return x+k;
    return x;
}


/// Clear mem (unused)
CudaGlobalFunction void clearmem(real_t* ptr) {
	ptr[CudaBlock.x] = 0.0;
}

/// Init Settings with 0 in GPU constant memory
void initSettings() {
	real_t val = 0;
<?R for (v in rows(Settings)) {
	if (is.na(v$derived)) { ?>
			CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &val, sizeof(real_t)); <?R
	}} ?>
}

/// Set Setting in GPU constant memory
/**
  Sets a Setting in the constant memory of GPU
  \param i Index of the Setting
  \param tmp value of the Setting
*/
void setConstSetting(int i, real_t tmp) {
	switch (i) {
<?R
        for (v in rows(Settings)) if (is.na(v$derived)) { ?>
	case <?%s v$Index ?>:
	        CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &tmp, sizeof(real_t));
		break; <?R
        } ?>
	}
}

/// Get maximal number of threads for all the kernels on runtime
template < class N, class Accessor > dim3 GetThreads() {
	dim3 ret;
	CudaFuncAttributes * attr = new CudaFuncAttributes;
	CudaFuncGetAttributes(attr, (RunKernel< N, Accessor >)) ;
	debug1( "[%d] Constant mem:%ld\n", D_MPI_RANK, attr->constSizeBytes);
	debug1( "[%d] Local    mem:%ld\n", D_MPI_RANK, attr->localSizeBytes);
	debug1( "[%d] Max  threads:%d\n", D_MPI_RANK, attr->maxThreadsPerBlock);
	debug1( "[%d] Reg   Number:%d\n", D_MPI_RANK, attr->numRegs);
	debug1( "[%d] Shared   mem:%ld\n", D_MPI_RANK, attr->sharedSizeBytes);
	if (attr->maxThreadsPerBlock > MAX_THREADS) attr->maxThreadsPerBlock = MAX_THREADS;
	ret.x = X_BLOCK;
	ret.y = attr->maxThreadsPerBlock/ret.x;
	ret.z = 1;
	return ret;
}

template < class N, class Accessor > class ThreadNumber {
public:
static unsigned int ky, sy, maxy;
static std::string name;
static void Init(bool fit, size_t ny, std::string name_) {
    ny = ny - 2;
      name = name_;
  dim3 th = GetThreads< N, Accessor >();
  maxy = th.y;
  sy = maxy;
  ky = ny/sy;
  
//	if (fit) while ((sy > 1) && (ky*sy != ny)) { sy--; ky = ny/sy; }
  if (fit) { sy = 1; ky = ny; }
  if (ky*th.y < ny) ky++;
  print();
}
static inline int getKY() { return ky; }
static inline int getSY() { return sy; }
static inline void print() {
  if (maxy != sy) {
  notice( "  %3dx%-3d  | %s --- Reduced from %d to fit Y dim\n", X_BLOCK, sy, name.c_str(), maxy);
  } else {
  output( "  %3dx%-3d  | %s\n", X_BLOCK, sy, name.c_str());
  }
}
};

template < class N, class Accessor > unsigned int ThreadNumber< N, Accessor >::ky = 0;
template < class N, class Accessor > unsigned int ThreadNumber< N, Accessor >::sy = 0;
template < class N, class Accessor > unsigned int ThreadNumber< N, Accessor >::maxy = 0;
template < class N, class Accessor > std::string ThreadNumber< N, Accessor >::name = "";

/// Initialize Thread/Block number variables
template <class Accessor> int InitDim(int ny) {
	MPI_Barrier(MPMD.local);
        output( "  Threads  |      Action\n");
<?R	for (tp in rows(AllKernels)[order(AllKernels$adjoint)]) { ifdef(tp$adjoint) ?>
        ThreadNumber< Node_Run<Accessor, <?%s tp$TemplateArgs ?> >, Accessor >::Init(<?%s {if (Stages$particle[tp$Stage]) "true" else "false"} ?>, ny, "<?%s tp$TemplateArgs ?>"); <?R
  }; ifdef(); ?>
  
	MPI_Barrier(MPMD.local);
        return 0;
}

/// Allocation of a GPU memory Buffer
void * BAlloc(size_t size) {
    char * tmp = NULL;
      ALLOCPRINT1;
      #ifdef DIRECT_MEM
        CudaMallocHost( (void**)&tmp, size );
      #else
        CudaMalloc( (void**)&tmp, size );
      #endif
      ALLOCPRINT2;
      CudaMemset( tmp, 0, size ); 
      return (void *) tmp;
}

/// Preallocation of a buffer (combines allocation into one big allocation)
void BPreAlloc(void ** ptr, size_t size) {
    CudaMalloc( ptr, size );
}


/// NULL-safe free
inline void MyFree(void * ptr) {
    if (ptr != NULL) {
        #ifdef DIRECT_MEM
	    CudaFreeHost( ptr );
	#else
	    CudaFree( ptr );
	#endif
	ptr = NULL;
    }
}

// Main kernel for lattice internals - just calls the RunKernel function defined in the Accessor
template <class N, class Accessor> CudaGlobalFunction void RunKernel() {
  // use template keyword for disambiguation
  Accessor::template RunKernel<N>();
}

/// BORDER_Z defines if we need to take Z direction in consiredation border/interior
//   if a model is 2D everything in the Z direction is interior
<?R if (any(DensityAll$dz != 0)) {?>
  #define BORDER_Z // The model is 3D
  <?R } else { ?>
  // The model is 2D (no BORDER_Z)
  <?R } ?>

/// Border Kernel
/**
  iterates over border elements and runs them with RunElement function
*/
template <class N, class Accessor> CudaGlobalFunction void RunBorderKernel()
{
	Accessor::template RunBorderKernel<N>();
}

template <class N> CudaGlobalFunction void RunParticlesKernel()
{
    /*	N now;
	Particle p(0.0,0.0,0.0);
	size_t i = CudaBlock.x;
	if (i < constContainer.particle_data_size) {
		p.rad = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_R];
		p.pos.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+0];
		p.pos.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+1];
		p.pos.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+2];
		p.vel.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+0];
		p.vel.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+1];
		p.vel.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+2];
		p.angvel.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+0];
		p.angvel.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+1];
		p.angvel.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+2];
		p.force.x = 0;
		p.force.y = 0;
		p.force.z = 0;
		p.moment.x = 0;
		p.moment.y = 0;
		p.moment.z = 0;
                int pdx = floor(p.pos.x - p.rad) - constContainer.px;
                int pnx = ceil(p.pos.x + p.rad) + 1 - constContainer.px;
                if (pdx < 0) pdx = 0;
                if (pnx > constContainer.nx) pnx = constContainer.nx;
                int pdy = floor(p.pos.y - p.rad) - constContainer.py;
                int pny = ceil(p.pos.y + p.rad) + 1 - constContainer.py;
                if (pdy < 0) pdy = 0;
                if (pny > constContainer.ny) pny = constContainer.ny;
                int pdz = floor(p.pos.z - p.rad) - constContainer.pz;
                int pnz = ceil(p.pos.z + p.rad) + 1 - constContainer.pz;
                if (pdz < 0) pdz = 0;
                if (pnz > constContainer.nz) pnz = constContainer.nz;
                for (now.idx_.z = pdz; now.idx_.z < pnz; now.idx_.z++) {	
                for (now.idx_.y = pdy+CudaThread.y; now.idx_.y < pny; now.idx_.y+=CudaNumberOfThreads.y) {	
                for (now.idx_.x = pdx+CudaThread.x; now.idx_.x < pnx; now.idx_.x+=CudaNumberOfThreads.x) {
                        p.diff.x = p.pos.x - now.idx_.x - constContainer.px;
                        p.diff.y = p.pos.y - now.idx_.y - constContainer.py;
                        p.diff.z = p.pos.z - now.idx_.z - constContainer.pz;
                        p.dist = sqrt(p.diff.x*p.diff.x + p.diff.y*p.diff.y + p.diff.z*p.diff.z);
                        p.cvel.x = p.vel.x + p.angvel.y*p.diff.z - p.angvel.z*p.diff.y;
                        p.cvel.y = p.vel.y + p.angvel.z*p.diff.x - p.angvel.x*p.diff.z;
                        p.cvel.z = p.vel.z + p.angvel.x*p.diff.y - p.angvel.y*p.diff.x;
                        now.RunElement(p);
//                    p.force.x += -p.vel.x;
//                    p.force.y += -p.vel.y;
        //		now.LoadElement();
        //		now.ExecElement(p);
        //		now.SaveElement(); 
                }
                }
                }
		real_t fx = blockSum(p.force.x);
		real_t fy = blockSum(p.force.y);
		real_t fz = blockSum(p.force.z);
		real_t afx = blockSum(p.moment.x);
		real_t afy = blockSum(p.moment.y);
		real_t afz = blockSum(p.moment.z);

//	fz = 0;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+0] = p.force.x;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+1] = p.force.y;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+2] = p.force.z;
		if (CudaThread.x == 0 && CudaThread.y == 0) {
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+0] = fx;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+1] = fy;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+2] = fz;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+0] = afx;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+1] = afy;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+2] = afz;
		}


	}
	*/
}

/// Old function for graphics output
/**
  calculates the color for one node
*/
template <class N, class Accessor>
CudaDeviceFunction void NodeToColor( int x, int y, int z, uchar4 *optr )
{
    int offset = x+y*Accessor::container().nx;
    float l=0.0; float w=0.0;
    int r=0,g=0,b=0;
    N now;
    if (x < 0) return;
    if (y < 0) return;
    if (z < 0) return;
    if (x >= Accessor::container().nx) return;
    if (y >= Accessor::container().ny) return;
    if (z >= Accessor::container().nz) return;
    now.idx_.x = x;
    now.idx_.y = y;
    now.idx_.z = z;
    Accessor::container().getType(now);
    Accessor::container().pop(now);
    {
     float2 v = now.Color();
     l = v.x;
     w = v.y;
    }

if (ISFINITE(l)) {

    l = l * 111;
    if (               (l <-111)) {r = 255; g = 255; b = 255; }
    if ((l >= -111) && (l < -11)) {r = 255*(-l-11)/100; g = 255; b = 255; }
    if ((l >=  -11) && (l <  -1)) {r = 0; g = (255*(-l-1))/10; b = 255; }
    if ((l >=   -1) && (l <   0)) {r = 0; g = 0; b = 255*(-l); }
    if ((l >=    0) && (l <   1)) {r = 255*l; g = 0; b = 0; }
    if ((l >=    1) && (l <  11)) {r = 255; g = 255*(l-1)/10; b = 0; }
    if ((l >=   11) && (l < 111)) {r = 255; g = 255; b = 255*(l-11)/100; }
    if ((l >=  111)             ) {r = 255; g = 255; b = 255; }
    r=r*w;
    g=g*w + (1-w)*255;
    b=b*w;
} else {
    r=255;
    b=255;
    g=0;
}
    optr[offset].x = r;  
    optr[offset].y = g;  
    optr[offset].z = b;  
    optr[offset].w = 255;
}

/// Kernel for graphics output
template <class Accessor> CudaGlobalFunction void ColorKernel( uchar4 *optr, int z )
{
  NodeToColor< Node_Run< Accessor, Primal, NoGlobals, Get >, Accessor >(
    CudaThread.x+CudaBlock.x*CudaNumberOfThreads.x,
    CudaBlock.y,
    z,
    optr
  );
}

// Functions for getting quantities
<?R
        for (q in rows(Quantities))
        {
                ifdef(q$adjoint); ?>
/// Calculate quantity [<?%s q$comment ?>] kernel
/**
  Kernel to calculate quantity <?%s q$name ?> (<?%s q$comment ?>) over a region
  For now, we add the option latticeType to pass in if we are using arbitrary or cartesian lattice due to our trouble templating this function. 
  We will have to write a completely different function for this for each lattice anyway, so no worries. Just a temporary measure until we develop the ArbitraryLattice.
  \param r Lattice region to calculate the quantity
  \param tab buffor to put the calculated result
  \param scale Scale to rescale the result (for units)
*/
CudaGlobalFunction void get<?%s q$name ?>(lbRegion r, <?%s q$type ?> * tab, real_t scale, int latticeType)
{
	int x = CudaBlock.x+r.dx;
  int y = CudaBlock.y+r.dy; 
  // please ignore the horrible code duplication here - had issues passing in the template arguments correctly. Will likely split this out later
  if(latticeType == 0) { // cartesian lattice
    <?R
    if (q$adjoint) { ?>
      Node_Run< CartesianLatticeContainerAccessor, Adjoint, NoGlobals, Get > now; <?R
    } else { ?>
      Node_Run< CartesianLatticeContainerAccessor, Primal, NoGlobals, Get > now; <?R
    }?>
    int z;
    for (z = r.dz; z < r.dz + r.nz; z ++) {
      now.idx_.x = x;
      now.idx_.y = y;
      now.idx_.z = z;

      constContainer.getType(now);
      <?%s q$type ?> w;
//		if (now.NodeType) {
      constContainer.pop(now);<?R
      if (q$adjoint) { ?>
        constContainer.pop_adj(now); <?R
                    } ?>
        w = now.get<?%s q$name ?>(); <?R
        if (q$type == "vector_t") {
                      for (coef in c("x","y","z")) { ?>
        w.<?%s coef ?> *= scale; <?R
          }
        } else { ?>
        w *= scale; <?R
                    } ?>
  //		} else { <?R
        if (q$type == "vector_t") {
                      for (coef in c("x","y","z")) { ?>
  //			w.<?%s coef ?> = nan(""); <?R
          }
        } else { ?>
  //			w = nan(""); <?R
                    } ?>
  //		}
      tab[r.offset(x,y,z)] = w;
    }

  } else if(latticeType == 1) { // arbitrary lattice
    <?R
    if (q$adjoint) { ?>
      Node_Run< ArbitraryLatticeContainerAccessor, Adjoint, NoGlobals, Get > now; <?R
    } else { ?>
      Node_Run< ArbitraryLatticeContainerAccessor, Primal, NoGlobals, Get > now; <?R
    }?>
      // compute the node id from the cuda block coordinates
      now.idx_.id = CudaBlock.x + (CudaBlock.y * constArbitraryContainer.getDimx);
      // check that we haven't gone off the lattice
      if(now.idx_.id < constArbitraryContainer.latticeSize){
        constArbitraryContainer.getType(now);
        <?%s q$type ?> w;
  //		if (now.NodeType) {
        constArbitraryContainer.pop(now);<?R
        if (q$adjoint) { ?>
          constArbitraryContainer.pop_adj(now); <?R
                      } ?>
          w = now.get<?%s q$name ?>(); <?R
          if (q$type == "vector_t") {
                        for (coef in c("x","y","z")) { ?>
          w.<?%s coef ?> *= scale; <?R
            }
          } else { ?>
          w *= scale; <?R
                      } ?>
    //		} else { <?R
          if (q$type == "vector_t") {
                        for (coef in c("x","y","z")) { ?>
    //			w.<?%s coef ?> = nan(""); <?R
            }
          } else { ?>
    //			w = nan(""); 
        <?R
                      } ?>
    //		}
        tab[now.idx_.id] = w;
        
      }
      // do nothing if we are off the lattice
  }
}
<?R
        }
        ifdef();
?>
