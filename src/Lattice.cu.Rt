#include "cross.h"
#include "types.h"
#include "Global.h"
#include "Node.h"
#include "LatticeContainer.h"
#include "Lattice.h"
#include "mpi.h"
#include "assert.h"

<?R
	source("conf.R")
?>

CudaExternConstantMemory(LatticeContainer constContainer);

int Lattice::Offset(int x, int y, int z)
{
	return x+region.nx*y + region.nx*region.ny*z;
}

Lattice::Lattice(lbRegion _region):region(_region)
{
	container = new LatticeContainer;
	container->Alloc(_region.nx,_region.ny,_region.nz);
	for (int i=0; i < <?%d nrow(Settings) ?>; i++) {
		settings[i] = 0.0;
	}
        CudaStreamCreate(&kernelStream);
        CudaStreamCreate(&inStream);
        CudaStreamCreate(&outStream);
	for (int i=0; i < 4; i++)
	        CudaEventCreate( &kernelEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &outEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &inEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &mpiEv[i] );
}

void Lattice::MPIInit(MPIInfo mpi_)
{
	mpi = mpi_;
//--------- Initialize MPI buffors
	bufnumber = 0;
#ifndef DIRECT_MEM
	printf("[%d] Allocating MPI buffors ...\n", D_MPI_RANK);
	type_f * ptr = NULL;
	int size, from, to;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
<?R
	for (i in NonEmptyMargin) {
?>
	size = <?R C(MarginSize[i],float=F) ?> * sizeof(type_f);
	from = mpi.node[mpi.rank].<?%s Margin$side[28-i] ?>;
	to = mpi.node[mpi.rank].<?%s Margin$side[i] ?>;
	if (mpi.rank != to) {
		CudaMallocHost(&ptr,size);
		mpiout[bufnumber] = ptr;
		gpuout[bufnumber] = container->out.<?%s Margin$name[i] ?>;
		nodeout[bufnumber] = to;
		CudaMallocHost(&ptr,size);
		mpiin[bufnumber] = ptr;
		gpuin[bufnumber] = container->in.<?%s Margin$name[i] ?>;
		nodein[bufnumber] = from;
		bufsize[bufnumber] = size;
		bufnumber ++;
	}
<?R
	}
?>
#endif

	printf("[%d] Done (BUFS: %d)\n", D_MPI_RANK, bufnumber);
#ifdef GPU_CROSS_IND
	printf("[%d] Starting MPI cross-indetification\n", D_MPI_RANK);
	{
		char to_name[MPI_MAX_PROCESSOR_NAME], from_name[MPI_MAX_PROCESSOR_NAME];
		MPI_Status status;
		{
			int len;
			MPI_Get_processor_name(from_name,&len);
		}
		printf("[%d] Running on: %s\n",D_MPI_RANK, from_name);
		for (to = 0; to < mpi.size; to++)
		for (from = 0; from < mpi.size; from++)
		if (to != from) {
		if ((to == mpi.rank) || (from == mpi.rank)) {
			int sameproc = 0, gpu = -1;
			if (to   == mpi.rank) {
				MPI_Send(from_name, MPI_MAX_PROCESSOR_NAME, MPI_BYTE, from, 0, MPI_COMM_WORLD);
				MPI_Send(&mpi.gpu,1,MPI_INT,from,1,MPI_COMM_WORLD);
			}
			if (from == mpi.rank) {
				MPI_Recv(to_name,   MPI_MAX_PROCESSOR_NAME, MPI_BYTE, to,   0, MPI_COMM_WORLD, &status);
				MPI_Recv(&gpu,1,MPI_INT,to,1,MPI_COMM_WORLD,&status);
				printf("[%d] recived from [%d]: %s\n",D_MPI_RANK, to, to_name);
				if (strcmp(to_name,from_name) == 0) {
					printf("[%d] and [%d] are on the same machine (GPU%d, GPU%d)\n",D_MPI_RANK, to, mpi.gpu, gpu);
				//	if (gpu != mpi.gpu) {
						sameproc = 1;
				//	}
				}
				MPI_Send(&sameproc,1,MPI_INT,to,0,MPI_COMM_WORLD);
			}
			if (to   == mpi.rank) MPI_Recv(&sameproc, 1, MPI_INT, from, 0, MPI_COMM_WORLD, &status);
			if (sameproc) for (int i = 0; i < bufnumber; i++) {
				if ((to ==   mpi.rank) && (from == nodein[i])) {
					// I will be reciving data from 'from' and putting it in 'in'
					MPI_Send(&gpuin[i],sizeof(type_f*),MPI_BYTE,from,i,MPI_COMM_WORLD);
					nodein[i] = -1;
				}
				if ((to == nodeout[i]) && (from ==  mpi.rank)) {
					// I will be sending data from 'out' to 'to'
					type_f * ptr;
					MPI_Recv(&ptr,sizeof(type_f*),MPI_BYTE,to,i,MPI_COMM_WORLD,&status);
					printf("[%d] recv (i:%d) buffor: %p on GPU%d\n",D_MPI_RANK,i,ptr,gpu);
					nodeout[i] = -(1+gpu);
					mpiout[i] = ptr;
				}
			}
		} MPI_Barrier(MPI_COMM_WORLD); }
		printf("[%d] Done (BUFS: %d)\n", D_MPI_RANK, bufnumber);
	}
#endif
}

void Lattice::listTabs(int*n, size_t ** size, void *** ptr, size_t * maxsize) {
	int j=0;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
	*maxsize = 0;
	*n = <?%d length(NonEmptyMargin) ?> + 1;
	*size = new size_t[*n];
	*ptr = new void*[*n];
	(*size)[j] = (size_t) nx*ny*nz*<?%d nrow(DensityAll) ?>*sizeof(type_f);
	(*ptr)[j] = (void*) container->in.f;
	if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;	
<?R
	for (i in NonEmptyMargin) {
?>
	(*size)[j] = (size_t) <?R C(MarginSize[i],float=F) ?> * sizeof(type_f);
	(*ptr)[j] = (void*) container->in.<?%s Margin$name[i] ?>;
	if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;
<?R
	}
?>
}

int Lattice::save(char * filename) {
	FILE * f = fopen(filename, "w");
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(&n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		CudaMemcpy( pt, ptr[i], size[i], cudaMemcpyDeviceToHost);
		fwrite(pt, size[i], 1, f);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

int Lattice::load(char * filename) {
	FILE * f = fopen(filename, "r");
	printf("[%d] Loading Lattice data from %s\n", D_MPI_RANK, filename);
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(&n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		fread(pt, size[i], 1, f);
		CudaMemcpy( ptr[i], pt, size[i], cudaMemcpyHostToDevice);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

Lattice::~Lattice()
{
	container->Free();
}

void Lattice::Color(uchar4 * ptr) {
	container->CopyToConst();
	CudaKernelRun( nodes_to_color , BLOCKS , THREADS ,(ptr));
}

void Lattice::Init()
{
   printf("[%d] Initializing Lattice ...\n", D_MPI_RANK);
//   CudaCopyToConstant("constContainer", constContainer, container, sizeof(LatticeContainer));
//   printf("[%d] Sizeof(LatticeContainer) = %ld\n", D_MPI_RANK, sizeof(LatticeContainer));
//   CudaKernelRun( nodes_fill , BLOCKS , THREADS ,(region.nx,region.ny));
   container->CopyToConst();
   container->Init();
   Stream();
   container->Init();
   Stream();
}

void Lattice::BeforeIt()
{
}

void Lattice::IterateT(int iter_type)
{
	type_f * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	int nx __attribute__ ((unused)) = region.nx;
	int ny __attribute__ ((unused)) = region.ny;
	int nz __attribute__ ((unused)) = region.nz;
	DEBUG_M;
	CudaEventRecord(kernelEv[0],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunBorder (kernelStream); break;
	case ITER_GLOBS:
		container->RunBorderG(kernelStream); break;
	}	
	DEBUG_M;
	CudaEventRecord(kernelEv[1],kernelStream);
	DEBUG_M;
	CudaStreamWaitEvent(outStream, kernelEv[1], 0);
        CudaEventRecord( outEv[0], outStream );
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], cudaMemcpyDeviceToHost, outStream);
	        CudaEventRecord( outEv[i], outStream );
	}
	CudaEventRecord(kernelEv[2],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunInterior (kernelStream); break;
	case ITER_GLOBS:
		container->RunInteriorG(kernelStream); break;
	}	
	DEBUG_M;
	CudaEventRecord(kernelEv[3],kernelStream);
	DEBUG_M;
        CudaStreamSynchronize(outStream);
	DEBUG_M;
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		DEBUG_M;
//		CudaEventSynchronize(outEv[i]);
		DEBUG_M;
		MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], mpi.rank*mpi.size + nodeout[i], MPI_COMM_WORLD, &request);
	}
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] < 0) {
#ifdef GPU_CROSS_IND
		printf("[%d] Device to Device copy %d:%p -> %d:%p\n",D_MPI_RANK, mpi.gpu, gpuout[i], -nodeout[i]-1, mpiout[i]);
	        CudaEventRecord( outEv[i], outStream );
		CudaMemcpyPeerAsync(mpiout[i],-nodeout[i]-1, gpuout[i], mpi.gpu, bufsize[i], outStream);
	        CudaEventRecord( mpiEv[i], inStream );
	        CudaEventRecord( inEv[i], outStream );
#else
		printf("[%d] Wrong node number in Stream!", D_MPI_RANK);
		DEBUG_M;
#endif
	}
	for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
		DEBUG_M;
		MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*mpi.size + mpi.rank, MPI_COMM_WORLD, &status);
	        CudaEventRecord( mpiEv[i], inStream );
		DEBUG_M;
		CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
		DEBUG_M;
	        CudaEventRecord( inEv[i], inStream );
	}
<?R
	for (i in NonEmptyMargin) if ((Margin$dy[i] != 0) || (Margin$dz[i] != 0)) {
?>
	from = mpi.rank;
	to = mpi.node[mpi.rank].<?%s Margin$side[i] ?>;
	if (to == mpi.rank) {
		tmp = container->in.<?%s Margin$name[i] ?>;
	    	container->in.<?%s Margin$name[i] ?> = container->out.<?%s Margin$name[i] ?>;
	    	container->out.<?%s Margin$name[i] ?> = tmp;
	}
	DEBUG_M;
<?R
	}
	for (i in NonEmptyMargin) if ((Margin$dy[i] == 0) && (Margin$dz[i] == 0)) {
?>
	tmp = container->in.<?%s Margin$name[i] ?>;
    	container->in.<?%s Margin$name[i] ?> = container->out.<?%s Margin$name[i] ?>;
    	container->out.<?%s Margin$name[i] ?> = tmp;
<?R
	}
?>
	tmp = container->in.f;
	container->in.f = container->out.f;
	container->out.f = tmp;

        CudaStreamSynchronize(kernelStream);
	DEBUG_M;
        CudaStreamSynchronize(inStream);
	DEBUG_M;
        CudaStreamSynchronize(outStream);
	DEBUG_M;
        CudaDeviceSynchronize();
	DEBUG_M;
	container->CopyToConst();
	DEBUG_M;
//	AfterIt();
	DEBUG_M;
};

void Lattice::AfterIt()
{
	float t;
	for (int i=0; i < 4; i++) {
		HANDLE_ERROR(CudaEventQuery(kernelEv[i]));
		CudaEventElapsedTime(&t, kernelEv[0], kernelEv[i]);
		printf("[%d] kernelEv[%d] --> %f\n", D_MPI_RANK, i, t);
	}
	for (int i = 0; i < bufnumber; i++) {
<?R
	for (Ev in c("outEv","mpiEv","inEv")) {
?>
		HANDLE_ERROR(CudaEventQuery( <?%s Ev ?>[i] ));
		CudaEventElapsedTime(&t, kernelEv[0], <?%s Ev ?>[i]);
		printf("[%d] <?%s Ev ?>[%d] --> %f\n", D_MPI_RANK, i, t);
<?R } ?>
	}

}

void Lattice::FlagOverwrite(flag_t * mask, lbRegion over)
{
//	printf("Overwriting flags of mesh (%lud b)\n", region.nx*region.ny*sizeof(flag_t));
//	CudaMemcpy2D(container->B, sizeof(flag_t), mask, sizeof(flag_t), sizeof(flag_t), region.nx*region.ny, cudaMemcpyHostToDevice);
	lbRegion inter = region.intersect(over);
//        printf("Overwriting flags of mesh (%ld b) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
                CudaMemcpy2D(&container->B[region.offsetL(x,y,z)], sizeof(flag_t), &mask[over.offsetL(x,y,z)], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyHostToDevice);
        }

}

/*

void Lattice::GetRegion(lbRegion over, Node * tab)
{
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(char), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&tab[over.offsetL(x,y,z)].B, sizeof(Node), &container->B[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
                for (int j=0; j<19; j++)
                {
//			printf("GetRegion: %d %d %d %d\n", x,y,z,j);
			CudaMemcpy2D(&tab[over.offsetL(x,y,z)].f[j], sizeof(Node), &container->in.f[offset], sizeof(type_f), sizeof(type_f), inter.nx, cudaMemcpyDeviceToHost);
			offset += region.sizeL();
		}
	}

}
*/

void Lattice::GetFlags(lbRegion over, flag_t * B)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&B[over.offsetL(x,y,z)], sizeof(flag_t), &container->B[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
	}

}

<?R for (i in 1:nrow(DensityAll)) { d = DensityAll[i,,drop=F]; ?>

void Lattice::Get_<?%s d$nicename ?>(lbRegion over, type_f * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(type_f),
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(type_f),
			inter.nx*sizeof(type_f),
			inter.ny,
			cudaMemcpyDeviceToHost);
	}
}

void Lattice::Set_<?%s d$nicename ?>(lbRegion over, type_f * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
        printf("Setting <?%s d$nicename ?> in region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("%dx%dx%d+%d,%d,%d\n", inter.nx,inter.ny,1, x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(type_f),
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(type_f),
			inter.nx*sizeof(type_f),
			inter.ny,
			cudaMemcpyHostToDevice);
	}
}

<?R } ?>

<?R for (i in 1:nrow(Quantities)) { q = Quantities[i,,drop=F]; ifdef(q$adjoint); ?>

void Lattice::Get<?%s q$name ?>(lbRegion over, <?%s q$type ?> * tab, type_f scale)
{
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s q$type ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s q$type ?>));
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?> , dim3(small.nx,small.ny) , dim3(1) , (small, buf, scale));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s q$type ?>), cudaMemcpyDeviceToHost);
	}
	CudaFree(buf);
}

<?R tp = "double" ?>
void Lattice::Get<?%s q$name ?>_<?%s tp ?>(lbRegion over, <?%s tp ?> * tab, int row)
{
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s tp ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s tp ?>)<?R if (q$type == "type_v") { ?>*3<?R }?>);
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?>_<?%s tp ?>, dim3(small.nx,small.ny) , dim3(1) , (small, buf, row));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		<?R 
	if (q$type == "type_v")
	{ ?>
		if (row == 0) {
			CudaMemcpy(&tab[over.offsetL(x,y,z)*3], buf, small.sizeL()*sizeof(<?%s tp ?>)*3, cudaMemcpyDeviceToHost);
		} else {
			for (int i = 0; i < 3; i++)
			CudaMemcpy(&tab[over.offsetL(x,y,z) + over.sizeL()*i], &buf[small.sizeL()*i], small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		} <?R
	} else { ?>
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		<?R
	} ?>
	}
	CudaFree(buf);
//	printf("%lf\n",tab[0]);
}


<?R }; ifdef() ?>

  void Lattice::getGlobals(type_f * tab) {
        type_f tabl[<?%d nrow(Globals) ?>];
        container->getGlobals(tabl);
        MPI_Reduce(tabl, tab, <?%d nrow(Globals) ?>, MPI_TYPE_F, MPI_SUM, 0, MPI_COMM_WORLD);
  }

<?R
        for (i in 1:nrow(Settings)) {
                v = Settings[i,];
                if (is.na(v$derived)) {
                        tmp = paste(v$name,"_",sep="");
                } else {
                        tmp = v$name;
                }
        ?>

void Lattice::<?%s v$FunName ?>(type_f <?%s tmp ?>) {
	settings[<?%d i-1 ?>] = <?%s tmp ?>;
        printf("[%d] Settings [<?%s v$comment ?>] to %f\n", D_MPI_RANK, <?%s tmp ?>); <?R
                if (is.na(v$derived)) { ?>
        CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &<?%s tmp ?>, sizeof(type_f)); <?R
                } else {
                        i = which(as.character(Settings$name) == as.character(v$derived));
                        if (is.null(i)) stop(paste(v$derived, "was declared as derived of",v$name,". But", v$derived, "doesn't exist\n"));
                        sel = Settings[i[1],]; ?>
        <?%s sel$FunName ?>(<?%s v$equation ?>); <?R
                } ?>
} <?R
        } ?>

