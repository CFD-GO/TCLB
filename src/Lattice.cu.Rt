<?R
	source("conf.R")
?>
#include "cross.h"
#include "types.h"
#include "Global.h"
#include "Node.h"
#include "LatticeContainer.h"
#include "Lattice.h"
#include "mpi.h"
#include "assert.h"

// Declaration of constContainer - a lattice container used to run all kernels
CudaExternConstantMemory(LatticeContainer constContainer);

//Local (CPU) calculation of offset
int Lattice::Offset(int x, int y, int z)
{
	return x+region.nx*y + region.nx*region.ny*z;
}

//Constructor based on size
Lattice::Lattice(lbRegion _region):region(_region)
{
	container = new LatticeContainer;
	ftabs = new FTabs[2];
	container->Alloc(_region.nx,_region.ny,_region.nz);
	for (int i=0; i < 2; i++) {
		ftabs[i].Alloc(_region.nx,_region.ny,_region.nz);
	}
	container->in = ftabs[0];
	container->out = ftabs[1];
	
	for (int i=0; i < <?%d nrow(Settings) ?>; i++) {
		settings[i] = 0.0;
	}
        CudaStreamCreate(&kernelStream);
        CudaStreamCreate(&inStream);
        CudaStreamCreate(&outStream);
	for (int i=0; i < 4; i++)
	        CudaEventCreate( &kernelEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &outEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &inEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &mpiEv[i] );
}

//Initialization of MPI buffors
void Lattice::MPIInit(MPIInfo mpi_)
{
	mpi = mpi_;
//--------- Initialize MPI buffors
	bufnumber = 0;
#ifndef DIRECT_MEM
	printf("[%d] Allocating MPI buffors ...\n", D_MPI_RANK);
	real_t * ptr = NULL;
	int size, from, to;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
<?R
	for (m in NonEmptyMargin) {
?>
	size = <?R C(m$Size,float=F) ?> * sizeof(real_t);
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (mpi.rank != to) {
		CudaMallocHost(&ptr,size);
		mpiout[bufnumber] = ptr;
		gpuout[bufnumber] = container->out.<?%s m$name ?>;
		nodeout[bufnumber] = to;
		CudaMallocHost(&ptr,size);
		mpiin[bufnumber] = ptr;
		gpuin[bufnumber] = container->in.<?%s m$name ?>;
		nodein[bufnumber] = from;
		bufsize[bufnumber] = size;
		bufnumber ++;
	}
<?R
	}
?>
#endif

	printf("[%d] Done (BUFS: %d)\n", D_MPI_RANK, bufnumber);
}


void Lattice::listTabs(int*n, size_t ** size, void *** ptr, size_t * maxsize) {
	int j=0;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
	*maxsize = 0;
	*n = <?%d length(NonEmptyMargin) ?>;
	*size = new size_t[*n];
	*ptr = new void*[*n];
<?R
	for (m in NonEmptyMargin) {
?>
	(*size)[j] = (size_t) <?R C(m$Size,float=F) ?> * sizeof(real_t);
	(*ptr)[j] = (void*) container->in.<?%s m$name ?>;
	if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;
<?R
	}
?>
}

int Lattice::save(char * filename) {
	FILE * f = fopen(filename, "w");
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(&n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		CudaMemcpy( pt, ptr[i], size[i], cudaMemcpyDeviceToHost);
		fwrite(pt, size[i], 1, f);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

int Lattice::load(char * filename) {
	FILE * f = fopen(filename, "r");
	printf("[%d] Loading Lattice data from %s\n", D_MPI_RANK, filename);
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(&n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		fread(pt, size[i], 1, f);
		CudaMemcpy( ptr[i], pt, size[i], cudaMemcpyHostToDevice);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

Lattice::~Lattice()
{
	container->Free();
}

void Lattice::Color(uchar4 * ptr) {
	container->Color(ptr);
}

void Lattice::Init()
{
   printf("[%d] Initializing Lattice ...\n", D_MPI_RANK);
//   CudaCopyToConstant("constContainer", constContainer, container, sizeof(LatticeContainer));
//   printf("[%d] Sizeof(LatticeContainer) = %ld\n", D_MPI_RANK, sizeof(LatticeContainer));
//   CudaKernelRun( nodes_fill , BLOCKS , THREADS ,(region.nx,region.ny));
   container->CopyToConst();
   container->Init();
   Stream();
   container->Init();
   Stream();
}

void Lattice::BeforeIt()
{
}

void Lattice::IterateT(int iter_type)
{
	real_t * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	DEBUG_M;
	CudaEventRecord(kernelEv[0],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunBorder (kernelStream); break;
	case ITER_GLOBS:
		container->RunBorderG(kernelStream); break;
	}	
	DEBUG_M;
	CudaEventRecord(kernelEv[1],kernelStream);
	DEBUG_M;
	CudaStreamWaitEvent(outStream, kernelEv[1], 0);
        CudaEventRecord( outEv[0], outStream );
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], cudaMemcpyDeviceToHost, outStream);
	        CudaEventRecord( outEv[i], outStream );
	}
	CudaEventRecord(kernelEv[2],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunInterior (kernelStream); break;
	case ITER_GLOBS:
		container->RunInteriorG(kernelStream); break;
	}	
	DEBUG_M;
	CudaEventRecord(kernelEv[3],kernelStream);
	DEBUG_M;
        CudaStreamSynchronize(outStream);
	DEBUG_M;
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		DEBUG_M;
//		CudaEventSynchronize(outEv[i]);
		DEBUG_M;
		MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], mpi.rank*mpi.size + nodeout[i], MPI_COMM_WORLD, &request);
	}
	for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
		DEBUG_M;
		MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*mpi.size + mpi.rank, MPI_COMM_WORLD, &status);
	        CudaEventRecord( mpiEv[i], inStream );
		DEBUG_M;
		CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
		DEBUG_M;
	        CudaEventRecord( inEv[i], inStream );
	}
<?R
	for (m in NonEmptyMargin) if ((m$dy != 0) || (m$dz != 0)) {
?>
	from = mpi.rank;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (to == mpi.rank) {
		tmp = container->in.<?%s m$name ?>;
	    	container->in.<?%s m$name ?> = container->out.<?%s m$name ?>;
	    	container->out.<?%s m$name ?> = tmp;
	}
	DEBUG_M;
<?R
	}
	for (m in NonEmptyMargin) if ((m$dy == 0) && (m$dz == 0)) {
?>
	tmp = container->in.<?%s m$name ?>;
    	container->in.<?%s m$name ?> = container->out.<?%s m$name ?>;
    	container->out.<?%s m$name ?> = tmp;
<?R
	}
?>
        CudaStreamSynchronize(kernelStream);
	DEBUG_M;
        CudaStreamSynchronize(inStream);
	DEBUG_M;
        CudaStreamSynchronize(outStream);
	DEBUG_M;
        CudaDeviceSynchronize();
	DEBUG_M;
	container->CopyToConst();
	DEBUG_M;
//	AfterIt();
	DEBUG_M;
};

void Lattice::AfterIt()
{
	float t;
	for (int i=0; i < 4; i++) {
		HANDLE_ERROR(CudaEventQuery(kernelEv[i]));
		CudaEventElapsedTime(&t, kernelEv[0], kernelEv[i]);
		printf("[%d] kernelEv[%d] --> %f\n", D_MPI_RANK, i, t);
	}
	for (int i = 0; i < bufnumber; i++) {
<?R
	for (Ev in c("outEv","mpiEv","inEv")) {
?>
		HANDLE_ERROR(CudaEventQuery( <?%s Ev ?>[i] ));
		CudaEventElapsedTime(&t, kernelEv[0], <?%s Ev ?>[i]);
		printf("[%d] <?%s Ev ?>[%d] --> %f\n", D_MPI_RANK, i, t);
<?R } ?>
	}

}

void Lattice::FlagOverwrite(flag_t * mask, lbRegion over)
{
//	printf("Overwriting flags of mesh (%lud b)\n", region.nx*region.ny*sizeof(flag_t));
//	CudaMemcpy2D(container->NodeType, sizeof(flag_t), mask, sizeof(flag_t), sizeof(flag_t), region.nx*region.ny, cudaMemcpyHostToDevice);
	lbRegion inter = region.intersect(over);
//        printf("Overwriting flags of mesh (%ld b) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
                CudaMemcpy2D(&container->NodeType[region.offsetL(x,y,z)], sizeof(flag_t), &mask[over.offsetL(x,y,z)], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyHostToDevice);
        }

}

/*

void Lattice::GetRegion(lbRegion over, Node * tab)
{
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(char), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&tab[over.offsetL(x,y,z)].NodeType, sizeof(Node), &container->NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
                for (int j=0; j<19; j++)
                {
			printf("GetRegion: %d %d %d %d\n", x,y,z,j);
//			CudaMemcpy2D(&tab[over.offsetL(x,y,z)].f[j], sizeof(Node), &container->in.f[offset], sizeof(real_t), sizeof(real_t), inter.nx, cudaMemcpyDeviceToHost);
			offset += region.sizeL();
		}
	}

}
*/

void Lattice::GetFlags(lbRegion over, flag_t * NodeType)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&NodeType[over.offsetL(x,y,z)], sizeof(flag_t), &container->NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
	}

}

<?R for (i in 1:nrow(DensityAll)) { d = DensityAll[i,,drop=F]; ?>

void Lattice::Get_<?%s d$nicename ?>(lbRegion over, real_t * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
/*		CudaMemcpy2D(
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(real_t),
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(real_t),
			inter.nx*sizeof(real_t),
			inter.ny,
			cudaMemcpyDeviceToHost);
*/	}
}

void Lattice::Set_<?%s d$nicename ?>(lbRegion over, real_t * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
        printf("Setting <?%s d$nicename ?> in region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
		printf("%dx%dx%d+%d,%d,%d\n", inter.nx,inter.ny,1, x,y,z);
		offset = region.offsetL(x,y,z);
/*		CudaMemcpy2D(
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(real_t),
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(real_t),
			inter.nx*sizeof(real_t),
			inter.ny,
			cudaMemcpyHostToDevice);
*/	}
}

<?R } ?>

<?R for (i in 1:nrow(Quantities)) { q = Quantities[i,,drop=F]; ifdef(q$adjoint); ?>

void Lattice::Get<?%s q$name ?>(lbRegion over, <?%s q$type ?> * tab, real_t scale)
{
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s q$type ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s q$type ?>));
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?> , dim3(small.nx,small.ny) , dim3(1) , (small, buf, scale));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s q$type ?>), cudaMemcpyDeviceToHost);
	}
	CudaFree(buf);
}

<?R tp = "double" ?>
void Lattice::Get<?%s q$name ?>_<?%s tp ?>(lbRegion over, <?%s tp ?> * tab, int row)
{
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s tp ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s tp ?>)<?R if (q$type == "vector_t") { ?>*3<?R }?>);
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?>_<?%s tp ?>, dim3(small.nx,small.ny) , dim3(1) , (small, buf, row));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		<?R 
	if (q$type == "vector_t")
	{ ?>
		if (row == 0) {
			CudaMemcpy(&tab[over.offsetL(x,y,z)*3], buf, small.sizeL()*sizeof(<?%s tp ?>)*3, cudaMemcpyDeviceToHost);
		} else {
			for (int i = 0; i < 3; i++)
			CudaMemcpy(&tab[over.offsetL(x,y,z) + over.sizeL()*i], &buf[small.sizeL()*i], small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		} <?R
	} else { ?>
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		<?R
	} ?>
	}
	CudaFree(buf);
//	printf("%lf\n",tab[0]);
}


<?R }; ifdef() ?>

  void Lattice::getGlobals(real_t * tab) {
        real_t tabl[<?%d nrow(Globals) ?>];
        container->getGlobals(tabl);
        MPI_Reduce(tabl, tab, <?%d nrow(Globals) ?>, MPI_TYPE_F, MPI_SUM, 0, MPI_COMM_WORLD);
  }

<?R
        for (i in 1:nrow(Settings)) {
                v = Settings[i,];
                if (is.na(v$derived)) {
                        tmp = paste(v$name,"_",sep="");
                } else {
                        tmp = v$name;
                }
        ?>

void Lattice::<?%s v$FunName ?>(real_t <?%s tmp ?>) {
	settings[<?%d i-1 ?>] = <?%s tmp ?>;
        printf("[%d] Settings [<?%s v$comment ?>] to %f\n", D_MPI_RANK, <?%s tmp ?>); <?R
                if (is.na(v$derived)) { ?>
        CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &<?%s tmp ?>, sizeof(real_t)); <?R
                } else {
                        i = which(as.character(Settings$name) == as.character(v$derived));
                        if (is.null(i)) stop(paste(v$derived, "was declared as derived of",v$name,". But", v$derived, "doesn't exist\n"));
                        sel = Settings[i[1],]; ?>
        <?%s sel$FunName ?>(<?%s v$equation ?>); <?R
                } ?>
} <?R
        } ?>

