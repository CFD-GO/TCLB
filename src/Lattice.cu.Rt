<?R
	source("conf.R")
	c_header();
?>
#include "cross.h"
#include "types.h"
#include "Global.h"
//#include "Node.h"
#include "LatticeContainer.h"
#include "Lattice.h"
#include "mpi.h"
#include "assert.h"

// Declaration of constContainer - a lattice container used to run all kernels
CudaExternConstantMemory(LatticeContainer constContainer);

// C-crazed function for calculating number of zeros at a end of a number.
int SnapLevel(unsigned int i) {
	unsigned int j = 16;
	unsigned int w = 0;
	unsigned int k = 0xFFFFu;
	while(j) {
		if (i & k) {
			j = j >> 1;
			k = k >> j;
		} else {
			w = w + j;
			j = j >> 1;
			k = k << j;
		}
	}
	return w;
}

//Local (CPU) calculation of offset
int Lattice::Offset(int x, int y, int z)
{
	return x+region.nx*y + region.nx*region.ny*z;
}

//Constructor based on size
Lattice::Lattice(lbRegion _region):region(_region)
{
	reverse_save=0;
	if (reverse_save) {
		nSnaps=10;
	} else {
		nSnaps=2;
	}
	container = new LatticeContainer;
	Snaps = new FTabs[nSnaps];
	iSnaps = new int[maxSnaps];
	container->Alloc(_region.nx,_region.ny,_region.nz);
	for (int i=0; i < nSnaps; i++) {
		Snaps[i].Alloc(_region.nx,_region.ny,_region.nz);
	}
	for (int i=0; i < maxSnaps; i++) {	
		iSnaps[i]=-1;
	}
	container->in = Snaps[0];
	container->out = Snaps[1];
#ifdef ADJOINT
	aSnaps = new FTabs[2];
	aSnaps[0].Alloc(_region.nx,_region.ny,_region.nz);
	aSnaps[1].Alloc(_region.nx,_region.ny,_region.nz);
	container->adjout = aSnaps[0];
#endif
	for (int i=0; i < <?%d nrow(Settings) ?>; i++) {
		settings[i] = 0.0;
	}
        CudaStreamCreate(&kernelStream);
        CudaStreamCreate(&inStream);
        CudaStreamCreate(&outStream);
	for (int i=0; i < 4; i++)
	        CudaEventCreate( &kernelEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &outEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &inEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &mpiEv[i] );
}

//Initialization of MPI buffors
void Lattice::MPIInit(MPIInfo mpi_)
{
	mpi = mpi_;
//--------- Initialize MPI buffors
	bufnumber = 0;
#ifndef DIRECT_MEM
	printf("[%d] Allocating MPI buffors ...\n", D_MPI_RANK);
	real_t * ptr = NULL;
	int size, from, to;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
<?R
	for (m in NonEmptyMargin) {
?>
	size = <?R C(m$Size,float=F) ?> * sizeof(real_t);
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if ((mpi.rank != to) && (size > 0)) {
		CudaMallocHost(&ptr,size);
		mpiout[bufnumber] = ptr;
		gpuout[bufnumber] = container->out.<?%s m$name ?>;
		nodeout[bufnumber] = to;
		CudaMallocHost(&ptr,size);
		mpiin[bufnumber] = ptr;
//		gpuin[bufnumber] = container->in.<?%s m$name ?>;
		gpuin[bufnumber] = (real_t*)BAlloc(size);
		nodein[bufnumber] = from;
		bufsize[bufnumber] = size;
		bufnumber ++;
	}
<?R
	}
?>
#endif

	printf("[%d] Done (BUFS: %d)\n", D_MPI_RANK, bufnumber);
}




void Lattice::Iteration(int tab0, int tab1, int iter_type, int iter)
{
	real_t * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	int i=0;
//	printf("%d -> %d\n", tab0, tab1);
<?R
	for (m in NonEmptyMargin) {
?>
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (mpi.rank != to) {
		gpuout[i] = Snaps[tab0].<?%s m$name ?>;
		container->in.<?%s m$name ?> = gpuin[i];
		i ++;
	} else {
		container->in.<?%s m$name ?> = Snaps[tab0].<?%s m$name ?>;
	}
	container->out.<?%s m$name ?> = Snaps[tab1].<?%s m$name ?>;
<?R
	}
?>
	container->CopyToConst();
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], cudaMemcpyDeviceToHost, outStream);
	}
	switch(iter_type){
	case ITER_NORM:
		container->RunInterior (kernelStream); break;
	case ITER_GLOBS:
		container->RunInterior_Globs(kernelStream); break;
	}	
        CudaStreamSynchronize(outStream);
	for (int i = 0; i < bufnumber; i++) {
		MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], mpi.rank*mpi.size + nodeout[i]+bufsize[i], MPI_COMM_WORLD, &request);
	}
	for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
		MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*mpi.size + mpi.rank+bufsize[i], MPI_COMM_WORLD, &status);
		CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
	}
        CudaStreamSynchronize(inStream);
	switch(iter_type){
	case ITER_NORM:
		container->RunBorder (kernelStream); break;
	case ITER_GLOBS:
		container->RunBorder_Globs(kernelStream); break;
	}	
        CudaStreamSynchronize(kernelStream);
        CudaDeviceSynchronize();
	iSnaps[tab1] = iter;
	Iter = iter;
	Snap = tab1;
};

#ifdef ADJOINT
void Lattice::Iteration_Adj(int tab0, int tab1, int iter_type, int iter)
{
	real_t * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	int i=0;
//	printf("Adjoint %d -> %d\n", tab0, tab1);
<?R
	for (m in NonEmptyMargin) {
?>
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (mpi.rank != to) {
		gpuout[i] = aSnaps[tab0].<?%s m$name ?>;
		container->in.<?%s m$name ?> = gpuin[i];
		i ++;
	} else {
		container->in.<?%s m$name ?> = aSnaps[tab0].<?%s m$name ?>;
	}
	container->out.<?%s m$name ?> = Snaps[Snap].<?%s m$name ?>;
	container->adjout.<?%s m$name ?> = aSnaps[tab1].<?%s m$name ?>;
<?R
	}
?>
	container->CopyToConst();
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], cudaMemcpyDeviceToHost, outStream);
	        CudaEventRecord( outEv[i], outStream );
	}

	CudaEventRecord(kernelEv[2],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunInterior_Adj (kernelStream); break;
	case ITER_GLOBS:
		container->RunInterior_Globs_Adj(kernelStream); break;
	}	
        CudaStreamSynchronize(outStream);
	DEBUG_M;
	for (int i = 0; i < bufnumber; i++) {
		DEBUG_M;
		MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], mpi.rank*mpi.size + nodeout[i]+bufsize[i], MPI_COMM_WORLD, &request);
	}
	for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
		DEBUG_M;
		MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*mpi.size + mpi.rank+bufsize[i], MPI_COMM_WORLD, &status);
	        CudaEventRecord( mpiEv[i], inStream );
		DEBUG_M;
		CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
		DEBUG_M;
	        CudaEventRecord( inEv[i], inStream );
	}
        CudaStreamSynchronize(inStream);
	DEBUG_M;
	CudaEventRecord(kernelEv[0],kernelStream);
	DEBUG_M;
	switch(iter_type){
	case ITER_NORM:
		container->RunBorder_Adj (kernelStream); break;
	case ITER_GLOBS:
		container->RunBorder_Globs_Adj(kernelStream); break;
	}	
	DEBUG_M;
	CudaEventRecord(kernelEv[1],kernelStream);
	DEBUG_M;

        CudaStreamSynchronize(kernelStream);
	DEBUG_M;
        CudaStreamSynchronize(inStream);
	DEBUG_M;
        CudaStreamSynchronize(outStream);
	DEBUG_M;
        CudaDeviceSynchronize();
	DEBUG_M;
	container->CopyToConst();
	DEBUG_M;
//	AfterIt();
	DEBUG_M;
//	iSnaps[tab1] = iter;
//	Iter = iter;
//	Snap = tab1;
};
#endif

void Lattice::listTabs(FTabs& tab, int*n, size_t ** size, void *** ptr, size_t * maxsize) {
	int j=0;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
	*maxsize = 0;
	*n = <?%d length(NonEmptyMargin) ?>;
	*size = new size_t[*n];
	*ptr = new void*[*n];
<?R
	for (m in NonEmptyMargin) {
?>
	(*size)[j] = (size_t) <?R C(m$Size,float=F) ?> * sizeof(real_t);
	(*ptr)[j] = (void*) tab.<?%s m$name ?>;
	if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;
<?R
	}
?>
}

int Lattice::save(FTabs& tab, char * filename) {
	FILE * f = fopen(filename, "w");
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		CudaMemcpy( pt, ptr[i], size[i], cudaMemcpyDeviceToHost);
		fwrite(pt, size[i], 1, f);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

int Lattice::load(FTabs& tab, char * filename) {
	FILE * f = fopen(filename, "r");
	printf("[%d] Loading Lattice data from %s\n", D_MPI_RANK, filename);
	if (f == NULL) {
		fprintf(stderr,"[%d] Cannot open %s for output\n", D_MPI_RANK, filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		fread(pt, size[i], 1, f);
		CudaMemcpy( ptr[i], pt, size[i], cudaMemcpyHostToDevice);
	}

	CudaFreeHost(pt);
//	printf("[%d] Maxsize:%d\n",D_MPI_RANK, maxsize);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

Lattice::~Lattice()
{
	container->Free();
	for (int i=0; i<nSnaps; i++) {
		Snaps[i].Free();
	}
	delete[] Snaps;	
	delete[] iSnaps;
}

void Lattice::Color(uchar4 * ptr) {
	container->Color(ptr);
}

void Lattice::Init()
{
   printf("[%d] Initializing Lattice ...\n", D_MPI_RANK);
//   CudaCopyToConstant("constContainer", constContainer, container, sizeof(LatticeContainer));
//   printf("[%d] Sizeof(LatticeContainer) = %ld\n", D_MPI_RANK, sizeof(LatticeContainer));
//   CudaKernelRun( nodes_fill , BLOCKS , THREADS ,(region.nx,region.ny));
	container->out=Snaps[0];
   container->CopyToConst();
   container->Init();
	iSnaps[getSnap(0)]=0;
	iSnaps[0]=0;
	Iter=0;
	Snap=0;

		{
			char filename[STRING_LEN];
			sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, getSnap(0));
			save(Snaps[0], filename);
		}

//   Stream();
//   container->Init();
//   Stream();
}

void Lattice::BeforeIt()
{
}

int Lattice::getSnap(int i) {
	int s = SnapLevel(i) + 1;
	return s;
}


void Lattice::IterateTill(int it, int iter_type)
{
	int s1,ls1,s2,ls2;
	int mx = -1, imx = -1, i;
	for (i=0;i<maxSnaps; i++) {
		if ((iSnaps[i] > mx) && (iSnaps[i] <= it)) {
			mx = iSnaps[i];
			imx = i;
		}
	}
	assert(imx >= 0);
//	printf("iterate: %d -> %d (%d primal) startSnap: %d\n", mx, it, it-mx, imx);
	if (imx >= nSnaps) {
		if (reverse_save) {
			printf("Reverse Adjoint Disk Read it:%d level:%d\n", mx, imx);
			char filename[STRING_LEN];
			sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, imx);
			load(Snaps[0], filename);
		} else {
			printf("Fatal Error: I have to read from disk, but reverse_save is not switched on\n");
			exit (-1);
		}
		iSnaps[0] = iSnaps[imx];
		imx = 0;
	}
	s1 = imx;
	for (i = mx; i < it; i++) {
		int s2 = getSnap(i+1);
		int s3 = s2;
		if (s2 >= nSnaps) s3=0;
		Iteration(s1, s3, iter_type, i+1);
		if (s2 >= nSnaps){
			if (reverse_save) {
				printf("Reverse Adjoint Disk Write it:%d level:%d\n", i+1, s2);
				char filename[STRING_LEN];
				sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, s2);
				save(Snaps[0], filename);
			}
		}
		iSnaps[s2] = i+1;
		iSnaps[s3] = i+1;
		s1 = s3;
	}
}

void Lattice::IterateT(int iter_type)
{
	static int i=0;
//	i--;
//	if (i < 0) i = 30000;
	i++;
	IterateTill(i,iter_type);
#ifdef ADJOINT
	Iteration_Adj(i % 2, (i+1) % 2, iter_type, i);
#endif
};

void Lattice::AfterIt()
{
	float t;
	for (int i=0; i < 4; i++) {
		HANDLE_ERROR(CudaEventQuery(kernelEv[i]));
		CudaEventElapsedTime(&t, kernelEv[0], kernelEv[i]);
		printf("[%d] kernelEv[%d] --> %f\n", D_MPI_RANK, i, t);
	}
	for (int i = 0; i < bufnumber; i++) {
<?R
	for (Ev in c("outEv","mpiEv","inEv")) {
?>
		HANDLE_ERROR(CudaEventQuery( <?%s Ev ?>[i] ));
		CudaEventElapsedTime(&t, kernelEv[0], <?%s Ev ?>[i]);
		printf("[%d] <?%s Ev ?>[%d] --> %f\n", D_MPI_RANK, i, t);
<?R } ?>
	}

}

void Lattice::FlagOverwrite(flag_t * mask, lbRegion over)
{
//	printf("Overwriting flags of mesh (%lud b)\n", region.nx*region.ny*sizeof(flag_t));
//	CudaMemcpy2D(container->NodeType, sizeof(flag_t), mask, sizeof(flag_t), sizeof(flag_t), region.nx*region.ny, cudaMemcpyHostToDevice);
	lbRegion inter = region.intersect(over);
//        printf("Overwriting flags of mesh (%ld b) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
                CudaMemcpy2D(&container->NodeType[region.offsetL(x,y,z)], sizeof(flag_t), &mask[over.offsetL(x,y,z)], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyHostToDevice);
        }

}

/*

void Lattice::GetRegion(lbRegion over, Node * tab)
{
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(char), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&tab[over.offsetL(x,y,z)].NodeType, sizeof(Node), &container->NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
                for (int j=0; j<19; j++)
                {
			printf("GetRegion: %d %d %d %d\n", x,y,z,j);
//			CudaMemcpy2D(&tab[over.offsetL(x,y,z)].f[j], sizeof(Node), &container->in.f[offset], sizeof(real_t), sizeof(real_t), inter.nx, cudaMemcpyDeviceToHost);
			offset += region.sizeL();
		}
	}

}
*/

void Lattice::GetFlags(lbRegion over, flag_t * NodeType)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&NodeType[over.offsetL(x,y,z)], sizeof(flag_t), &container->NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
	}

}

<?R for (i in 1:nrow(DensityAll)) { d = DensityAll[i,,drop=F]; ?>

void Lattice::Get_<?%s d$nicename ?>(lbRegion over, real_t * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        printf("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
		printf("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
/*		CudaMemcpy2D(
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(real_t),
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(real_t),
			inter.nx*sizeof(real_t),
			inter.ny,
			cudaMemcpyDeviceToHost);
*/	}
}

void Lattice::Set_<?%s d$nicename ?>(lbRegion over, real_t * tab)
{
//	printf("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
        printf("Setting <?%s d$nicename ?> in region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
	int y = inter.dy;
//        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
		printf("%dx%dx%d+%d,%d,%d\n", inter.nx,inter.ny,1, x,y,z);
		offset = region.offsetL(x,y,z);
/*		CudaMemcpy2D(
			&container->in.f[offset+<?%d i-1?>*region.sizeL()], region.nx*sizeof(real_t),
			&tab[over.offsetL(x,y,z)], inter.nx*sizeof(real_t),
			inter.nx*sizeof(real_t),
			inter.ny,
			cudaMemcpyHostToDevice);
*/	}
}

<?R } ?>

<?R for (i in 1:nrow(Quantities)) { q = Quantities[i,,drop=F]; ifdef(q$adjoint); ?>

void Lattice::Get<?%s q$name ?>(lbRegion over, <?%s q$type ?> * tab, real_t scale)
{
	container->out = Snaps[Snap];
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s q$type ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s q$type ?>));
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?> , dim3(small.nx,small.ny) , dim3(1) , (small, buf, scale));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s q$type ?>), cudaMemcpyDeviceToHost);
	}
	CudaFree(buf);
}

<?R tp = "double" ?>
void Lattice::Get<?%s q$name ?>_<?%s tp ?>(lbRegion over, <?%s tp ?> * tab, int row)
{
	lbRegion inter = region.intersect(over);
//        printf("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s tp ?> * buf;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s tp ?>)<?R if (q$type == "vector_t") { ?>*3<?R }?>);
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        printf("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?>_<?%s tp ?>, dim3(small.nx,small.ny) , dim3(1) , (small, buf, row));
//	        printf("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		<?R 
	if (q$type == "vector_t")
	{ ?>
		if (row == 0) {
			CudaMemcpy(&tab[over.offsetL(x,y,z)*3], buf, small.sizeL()*sizeof(<?%s tp ?>)*3, cudaMemcpyDeviceToHost);
		} else {
			for (int i = 0; i < 3; i++)
			CudaMemcpy(&tab[over.offsetL(x,y,z) + over.sizeL()*i], &buf[small.sizeL()*i], small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		} <?R
	} else { ?>
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s tp ?>), cudaMemcpyDeviceToHost);
		<?R
	} ?>
	}
	CudaFree(buf);
//	printf("%lf\n",tab[0]);
}


<?R }; ifdef() ?>

  void Lattice::getGlobals(real_t * tab) {
        real_t tabl[<?%d nrow(Globals) ?>];
        container->getGlobals(tabl);
        MPI_Reduce(tabl, tab, <?%d nrow(Globals) ?>, MPI_TYPE_F, MPI_SUM, 0, MPI_COMM_WORLD);
  }

<?R
        for (i in 1:nrow(Settings)) {
                v = Settings[i,];
                if (is.na(v$derived)) {
                        tmp = paste(v$name,"_",sep="");
                } else {
                        tmp = v$name;
                }
        ?>

void Lattice::<?%s v$FunName ?>(real_t <?%s tmp ?>) {
	settings[<?%d i-1 ?>] = <?%s tmp ?>;
        printf("[%d] Settings [<?%s v$comment ?>] to %f\n", D_MPI_RANK, <?%s tmp ?>); <?R
                if (is.na(v$derived)) { ?>
        CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &<?%s tmp ?>, sizeof(real_t)); <?R
                } else {
                        i = which(as.character(Settings$name) == as.character(v$derived));
                        if (is.null(i)) stop(paste(v$derived, "was declared as derived of",v$name,". But", v$derived, "doesn't exist\n"));
                        sel = Settings[i[1],]; ?>
        <?%s sel$FunName ?>(<?%s v$equation ?>); <?R
                } ?>
} <?R
        } ?>

