<?R
	source("conf.R")
	c_header();
?>
/*  File defining Lattice                                      */
/*     Lattice is the low level class defining functionality   */
/*       of Adjoint LBM solver. It realizes all the LBM        */
/*       calculations and data transfer                        */
/*-------------------------------------------------------------*/

#include "Consts.h"
#include "cross.h"
#include "types.h"
#include "Global.h"
#include "LatticeContainer.h"
#include "Lattice.h"
#include "mpi.h"
#include "assert.h"

/// LatticeContainer stored in the GPU const memory
/**
        This is the main container that is stored in the GPU const memory.
        It is used to pass all the needed information on buffers etc to the kernels
*/
CudaExternConstantMemory(LatticeContainer constContainer);

/// Calculate the Snapshot level for the optimal Checkpoiting technique
/**
        C-crazed function for calculating number of zeros at
        the end of a number written in a binary system
        /param i The number
        /return number of zeros at the end of the number written in a binary system
*/
int SnapLevel(unsigned int i) {
	unsigned int j = 16;
	unsigned int w = 0;
	unsigned int k = 0xFFFFu;
	while(j) {
		if (i & k) {
			j = j >> 1;
			k = k >> j;
		} else {
			w = w + j;
			j = j >> 1;
			k = k << j;
		}
	}
	return w;
}

/// Calculation of the offset from X, Y and Z
int Lattice::Offset(int x, int y, int z)
{
	return x+region.nx*y + region.nx*region.ny*z;
}

/// Constructor based on size
/**
        Main constructor of Lattice
        \param _region Local region of the Lattice
        \param mpi_ MPI Information
        \param ns Number of Snapshots
*/
Lattice::Lattice(lbRegion _region, MPIInfo mpi_, int ns):region(_region), mpi(mpi_)
{
	DEBUG_M;
	reverse_save=0;
	Record_Iter = 0;
	Iter = 0;
	total_iterations = 0;
	segment_iterations = 0;
	callback_iter = 1;
	nSnaps = ns;
	container = new LatticeContainer;
	sample = new Sampler;
	Snaps = new FTabs[nSnaps];
	iSnaps = new int[maxSnaps];
	container->Alloc(_region.nx,_region.ny,_region.nz);
	container->px = region.dx;
	container->py = region.dy;
	container->pz = region.dz;
	container->iter = 0;
	DEBUG_M;
	for (int i=0; i < nSnaps; i++) {
		Snaps[i].PreAlloc(_region.nx,_region.ny,_region.nz);
	}
	for (int i=0; i < maxSnaps; i++) {	
		iSnaps[i]= -1;
	}
#ifdef ADJOINT
	aSnaps = new FTabs[2];
	aSnaps[0].PreAlloc(_region.nx,_region.ny,_region.nz);
	aSnaps[1].PreAlloc(_region.nx,_region.ny,_region.nz);
#endif
	DEBUG_M;
	MPIInit(mpi);
	DEBUG_M;
	CudaAllocFinalize();
	DEBUG_M;

	container->in = Snaps[0];
	container->out = Snaps[1];
#ifdef ADJOINT
	container->adjout = aSnaps[0];
#endif
	aSnap = 0;
	for (int i=0; i < SETTINGS; i++) {
		settings[i] = 0.0;
	}
        CudaStreamCreate(&kernelStream);
        CudaStreamCreate(&inStream);
        CudaStreamCreate(&outStream);
	for (int i=0; i < 4; i++)
	        CudaEventCreate( &kernelEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &outEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &inEv[i] );
	for (int i=0; i < 28; i++)
	        CudaEventCreate( &mpiEv[i] );
        container->ZoneSettings = zSet.gpuTab;
        container->ConstZoneSettings = zSet.gpuConst;
        container->ZoneIndex = 0;
<?R for (s in rows(ZoneSettings)) { ?>
//        zSet.set(<?%s s$Index ?>, -1, units.alt("<?%s s$default ?>"));
<?R } ?>
        ZoneIter = 0;

}

/// Initialization of MPI buffors
/**
        Initialize all the buffors needed for the MPI data transfer
        \param mpi_ MPI Information (connectivity)
*/
void Lattice::MPIInit(MPIInfo mpi_)
{
//--------- Initialize MPI buffors
	bufnumber = 0;
#ifndef DIRECT_MEM
	debug2("Allocating MPI buffors ...\n");
	real_t * ptr = NULL;
	int size, from, to;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
<?R
	for (m in NonEmptyMargin) {
?>
	size = <?R C(m$Size,float=F) ?> * sizeof(real_t);
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if ((mpi.rank != to) && (size > 0)) {
		CudaMallocHost(&ptr,size);
		mpiout[bufnumber] = ptr;
		gpuout[bufnumber] = NULL; 
		nodeout[bufnumber] = to;
		CudaMallocHost(&ptr,size);
		mpiin[bufnumber] = ptr;
		BPreAlloc((void**) & (gpubuf[bufnumber]), size);
		BPreAlloc((void**) & (gpubuf2[bufnumber]), size);
		nodein[bufnumber] = from;
		bufsize[bufnumber] = size;
		bufnumber ++;
	}
<?R
	}
?>
#endif

	debug2("Done (BUFS: %d)\n", bufnumber);
}

/// Starting of unsteady adjoint recording
/**
        Starts tape recording of the iteration process including:
        all the iterations done
        changes of settings
*/
void Lattice::startRecord()
{
	if (reverse_save) {
		ERROR("Nested record! Called startRecord while recording\n");
		exit(-1);
	}
	if (Record_Iter != 0) {
		ERROR("Record tape is not rewinded (iter = %d) maybe last adjoint didn't go all the way\n", Record_Iter);
		Record_Iter = 0;
	}
	debug2("Lattice is starting to record (usteady adjoint)\n");
	if (Snap != 0) {
		warning("Snap = %d at startRecord\n", Snap);
	}
	for (int i=0; i < maxSnaps; i++) {	
		iSnaps[i]= -1;
	}
	{
		char filename[STRING_LEN];
		sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, getSnap(0));
		iSnaps[getSnap(0)] = 0;
		save(Snaps[Snap], filename);
	}
	if (Snap != 0) {
		warning("Snap = %d. Going through disk\n", Snap);
	} else {
		iSnaps[Snap] = 0;
	}
	reverse_save = 1;
	clearAdjoint();
	clearGlobals();
	settings_record.clear();
	settings_i=0;
}

/// Saves solution to binary files
/**
        Dump the primal and adjoint solutions to binary files
        \param filename Prefix/path for the dumped binary files
*/
void Lattice::saveSolution(const char * filename) {
	char fn[STRING_LEN];
	sprintf(fn, "%s_%d.pri", filename, D_MPI_RANK);
	save(Snaps[Snap], fn);
#ifdef ADJOINT
	sprintf(fn, "%s_%d.adj", filename, D_MPI_RANK);
	save(aSnaps[aSnap], fn);
#endif
}

/// Loades solution for binary files
/**
        Loades the primal and adjoint solutions from binary files
        \param filename Prefix/path for the dumped binary files
*/
void Lattice::loadSolution(const char * filename) {
	char fn[STRING_LEN];
	sprintf(fn, "%s_%d.pri", filename, D_MPI_RANK);
	if (load(Snaps[Snap], fn)) exit(-1);
#ifdef ADJOINT
	sprintf(fn, "%s_%d.adj", filename, D_MPI_RANK);
	load(aSnaps[aSnap], fn);
#endif
}

/// Clear the adjoint solution buffers
/**
        Clear the Adjoint Snapshots
*/
void Lattice::clearAdjoint()
{
#ifdef ADJOINT
	debug1("Clearing adjoint\n");
	aSnaps[0].Clear(region.nx,region.ny,region.nz);
	aSnaps[1].Clear(region.nx,region.ny,region.nz);
#endif
	zSet.ClearGrad();
}

/// Clear the derivatives of the parameters
/**
        Clear the derivative component/density of the adjoint solution
*/
void Lattice::clearDPar()
{ <?R
	for (d in rows(DensityAll)) if ((d$adjoint) && (d$parameter)) { ?>
	Clear_<?%s d$nicename ?>(); <?R
	}
?>
}

/// Finish unsteady adjoint record
/**
        Stops the Adjoint recording process
*/
void Lattice::stopRecord()
{
	if (Record_Iter != 0) {
		WARNING("Record tape is not rewinded (iter = %d)\n",Record_Iter);
		Record_Iter = 0;
	}
	reverse_save = 0;
	debug2("Stop recording\n");
}

/// Set monitor callback
/**
        Sets the monitor callback which will be called every second or frame
*/
void Lattice::Callback(int(*cb)(int,int, void*), void* data) {
	callback = cb;
	callback_data = data;
}

/// Copy GPU to CPU memory
inline void Lattice::MPIStream_A()
{
	for (int i = 0; i < bufnumber; i++) if (nodeout[i] >= 0) {
		CudaMemcpyAsync( mpiout[i], gpuout[i], bufsize[i], cudaMemcpyDeviceToHost, outStream);
	}
}

/// Copy Buffers between processors
inline void Lattice::MPIStream_B(int tag)
{
        if (bufnumber > 0) {
                DEBUG_M;
                CudaStreamSynchronize(outStream);
                DEBUG_M;
        #ifdef CROSS_MPI_OLD
                MPI_Status status;
                MPI_Request request;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], i+tag, MPI_COMM_WORLD, &request);
                }
                for (int i = 0; i < bufnumber; i++) if (nodein[i] >= 0) {
                        MPI_Recv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], nodein[i]*mpi.size + mpi.rank+bufsize[i], MPI_COMM_WORLD, &status);
                        CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
                }
        #else
                MPI_Status status;
                MPI_Request request;
//                MPI_Request recvreq[bufnumber];
                MPI_Request * recvreq = new MPI_Request[bufnumber];
                MPI_Request * sendreq = new MPI_Request[bufnumber];
        //	DEBUG_M;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Irecv( mpiin[i], bufsize[i], MPI_BYTE, nodein[i], i+tag, MPI_COMM_WORLD, &recvreq[i]);
                }
        //	DEBUG_M;
                for (int i = 0; i < bufnumber; i++) {
                        MPI_Isend( mpiout[i], bufsize[i], MPI_BYTE, nodeout[i], i+tag, MPI_COMM_WORLD, &sendreq[i]);
                }
        //	DEBUG_M;
                #ifdef CROSS_MPI_WAITANY
        //        	DEBUG_M;
                        for (int j = 0; j < bufnumber; j++) {
                                int i;
                                MPI_Waitany(bufnumber, recvreq, &i, MPI_STATUSES_IGNORE);
                                CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
                        }
                #else
                        DEBUG_M;
                        MPI_Waitall(bufnumber, recvreq, MPI_STATUSES_IGNORE);
                        DEBUG_M;
                        for (int i = 0; i < bufnumber; i++) {
                                CudaMemcpyAsync( gpuin[i], mpiin[i], bufsize[i], cudaMemcpyHostToDevice, inStream);
                        }
                #endif
		MPI_Waitall(bufnumber, sendreq, MPI_STATUSES_IGNORE);
                delete[] recvreq;
                delete[] sendreq;
        #endif
                DEBUG_M;
                CudaStreamSynchronize(inStream);
                DEBUG_M;
        }
}


<?R for (n in names(Actions)) {
        a = Actions[[n]]
        if (n == "Iteration") {
                FunName = "Iteration"
        } else {
                FunName = paste("Action",n,sep="_")
        } ?>
/// Normal (Primal) Iteration
/**
        One Primal Iteration
        \param tab0 Snapshot from which to start
        \param tab1 Snapshot in which to put result
        \param iter_type Type of the iteration
*/
inline void Lattice::<?%s FunName ?>(int tab0, int tab1, int iter_type)
{
	real_t * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	int i=0;
	debug1("Iteration %d -> %d type: %d. iter: %d\n", tab0, tab1, iter_type, Iter);
	ZoneIter = (Iter + Record_Iter) % zSet.getLen();

	debug1("ZoneIter: %d (in <?%s FunName ?>)\n", ZoneIter);
	container->ZoneIndex = ZoneIter;
	container->MaxZones = zSet.MaxZones;
	ST.Generate();
	ST.CopyToGPU(container->ST);
<?R
	for (m in NonEmptyMargin) {
?>
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (mpi.rank != to) {
		gpuin[i] = Snaps[tab1].<?%s m$name ?>;
		gpuout[i] = container->out.<?%s m$name ?> = gpubuf[i];
		i ++;
	} else {
		container->out.<?%s m$name ?> = Snaps[tab1].<?%s m$name ?>;
	}
	container->in.<?%s m$name ?> = Snaps[tab0].<?%s m$name ?>;
<?R
	}
	old_stage_level = 0
	for (s in a) {
                 stage = Stages[s,,drop=F] ?>
//---------------- STAGE: <?%s stage$name ?> ----------------------- <?R
                 if (old_stage_level > 0) { 
                	for (m in NonEmptyMargin) { ?>
	container->in.<?%s m$name ?> = Snaps[tab1].<?%s m$name ?>; <?R
                        } 
                 }
                 old_stage_level = old_stage_level + 1 ?>

<?R if (stage$fixedPoint) { ?> for (int fix=0; fix<100; fix++) { <?R } ?>
	container->CopyToConst();
	switch(iter_type & ITER_INTEG){
	case ITER_NO:
		container->RunBorder< Primal, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS:
		container->RunBorder< Primal, IntegrateGlobals, <?%s stage$name ?> >(kernelStream); break;
	case ITER_OBJ:
#ifdef ADJOINT
		container->RunBorder< Primal, OnlyObjective, <?%s stage$name ?> >(kernelStream);
#endif
                break;
	}	
        CudaStreamSynchronize(kernelStream);

        MPIStream_A();

	switch(iter_type & ITER_INTEG){
	case ITER_NO:
		container->RunInterior< Primal, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS:
		container->RunInterior< Primal, IntegrateGlobals, <?%s stage$name ?> >(kernelStream); break;
	case ITER_OBJ:
#ifdef ADJOINT
		container->RunInterior< Primal, OnlyObjective, <?%s stage$name ?> >(kernelStream);
#endif
                break;
	}	
	
	MPIStream_B();

        CudaDeviceSynchronize();
<?R if (stage$fixedPoint) { ?> } // for(fix) <?R } ?>
<?R } ?>
	Snap = tab1;
	MarkIteration();
	updateAllSamples();	
};

/// Adjoint Iteration
/**
        One Adjoint Iteration
        \param tab0 Adjoint Snapshot from which to start
        \param tab1 Adjoint Snapshot in which to put result
        \param iter_type Type of the iteration
*/
inline void Lattice::<?%s FunName ?>_Adj(int tab0, int tab1, int adjtab0, int adjtab1, int iter_type)
{
#ifdef ADJOINT
	real_t * tmp;
	int size, from, to;
	MPI_Status status;
	MPI_Request request;
	int i=0;
	debug1("[%d] Iteration_Adj %d -> %d type: %d\n", D_MPI_RANK, adjtab0, adjtab1, iter_type);
	ZoneIter = (Iter + Record_Iter) % zSet.getLen();
	container->ZoneIndex = ZoneIter;
	container->MaxZones = zSet.MaxZones;
	ST.Generate();
	ST.CopyToGPU(container->ST);

	debug1("ZoneIter: %d (in <?%s FunName ?>_Adj)\n", ZoneIter);
<?R
	for (m in NonEmptyMargin) {
?>
	from = mpi.node[mpi.rank].<?%s m$opposite_side ?>;
	to = mpi.node[mpi.rank].<?%s m$side ?>;
	if (mpi.rank != to) {
		gpuout[i] = aSnaps[adjtab0].<?%s m$name ?>;
		gpuin[i] = container->adjin.<?%s m$name ?> = gpubuf[i];
		i ++;
	} else {
		container->adjin.<?%s m$name ?> = aSnaps[adjtab0].<?%s m$name ?>;
	}
	container->in.<?%s m$name ?> = Snaps[tab0].<?%s m$name ?>;
	container->adjout.<?%s m$name ?> = aSnaps[adjtab1].<?%s m$name ?>;
<?R
	}

	for (s in a) {
                 stage = Stages[s,,drop=F] ?>

       	container->CopyToConst();

        MPIStream_A();

	switch(iter_type & ITER_INTEG){
	case ITER_NO:
	        container->RunInterior< Adjoint, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS:
	        container->RunInterior< Adjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_NO | ITER_STEADY:
	        container->RunInterior< SteadyAdjoint, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS | ITER_STEADY:
	        container->RunInterior< SteadyAdjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream); break;
	}	

        MPIStream_B();

	DEBUG_M;
	switch(iter_type & ITER_INTEG){
	case ITER_NO:
	        container->RunBorder< Adjoint, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS:
	        container->RunBorder< Adjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_NO | ITER_STEADY:
	        container->RunBorder< SteadyAdjoint, NoGlobals, <?%s stage$name ?> > (kernelStream); break;
	case ITER_GLOBS | ITER_STEADY:
	        container->RunBorder< SteadyAdjoint, IntegrateGlobals, <?%s stage$name ?> > (kernelStream); break;
	}	
	DEBUG_M;
        CudaDeviceSynchronize();
<?R } ?>
	aSnap = adjtab1;
	MarkIteration();
#else
	ERROR("This model doesn't have adjoint!\n");
	exit (-1);
#endif
};

/// Combined Primal+Adjoint Iteration
/**
        One combined Primal+Adjoint Iteration (steepest descent included)
        \param tab0 Snapshot from which to start
        \param tab1 Snapshot in which to put result
        \param adjtab0 Adjoint Snapshot from which to start
        \param adjtab1 Adjoint Snapshot in which to put result
        \param iter_type Type of the iteration
*/
inline void Lattice::<?%s FunName ?>_Opt(int tab0, int tab1, int adjtab0, int adjtab1, int iter_type)
{
#ifdef ADJOINT
        <?%s FunName ?>(tab0, tab1, iter_type);
        <?%s FunName ?>_Adj(tab0, tab1, adjtab0, adjtab1, iter_type | ITER_STEADY);
        container->RunInterior< Optimize, NoGlobals, <?%s stage$name ?> > (kernelStream);
        container->RunBorder< Optimize, NoGlobals, <?%s stage$name ?> > (kernelStream); 
        CudaDeviceSynchronize();
#else
	ERROR("This model doesn't have adjoint!\n");
	exit (-1);
#endif
};

<?R } ?>


/// Function listing all buffers in FTabs
void Lattice::listTabs(FTabs& tab, int*n, size_t ** size, void *** ptr, size_t * maxsize) {
	int j=0;
	int nx = region.nx, ny=region.ny,  nz=region.nz;
	*maxsize = 0;
	*n = <?%d length(NonEmptyMargin) ?>;
	*size = new size_t[*n];
	*ptr = new void*[*n];
<?R
	for (m in NonEmptyMargin) {
?>
	(*size)[j] = (size_t) <?R C(m$Size,float=F) ?> * sizeof(real_t);
	(*ptr)[j] = (void*) tab.<?%s m$name ?>;
	if ((*size)[j] > *maxsize) *maxsize = (*size)[j];
	j++;
<?R
	}
?>
}

/// Save a FTabs
int Lattice::save(FTabs& tab, const char * filename) {
	FILE * f = fopen(filename, "w");
	if (f == NULL) {
		ERROR("Cannot open %s for output\n", filename);
		assert(f == NULL);
		return -1;
	}

	void ** ptr;
	void * pt=NULL;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
        output("Saving data slice %d, size %d", i, size[i]);
		CudaMemcpy( pt, ptr[i], size[i], cudaMemcpyDeviceToHost);
		fwrite(pt, size[i], 1, f);
	}

	CudaFreeHost(pt);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

/// Load a FTabs
int Lattice::load(FTabs& tab, const char * filename) {
	FILE * f = fopen(filename, "r");
	output("Loading Lattice data from %s\n", filename);
	if (f == NULL) {
		ERROR("Cannot open %s for output\n", filename);
		return -1;
	}

	void ** ptr;
	void * pt = NULL;
	size_t * size;
	size_t maxsize;
	int n;

	listTabs(tab, &n, &size, &ptr, &maxsize);
	CudaMallocHost(&pt,maxsize);

	for(int i=0; i<n; i++)
	{
		int ret = fread(pt, size[i], 1, f);
		if (ret != 1) ERROR("Could not read in Lattice::load");
		CudaMemcpy( ptr[i], pt, size[i], cudaMemcpyHostToDevice);
	}

	CudaFreeHost(pt);
	fclose(f);
	delete[] size;
	delete[] ptr;
	return 0;
}

/// Destructor
/**
        I think it doesn't leave a big mess
*/
Lattice::~Lattice()
{
        CudaAllocFreeAll();
	container->Free();
	for (int i=0; i<nSnaps; i++) {
		Snaps[i].Free();
	}
	delete[] Snaps;	
	delete[] iSnaps;
}

/// Render Graphics (GUI)
/**
        Renders graphics in the GUI version
*/
void Lattice::Color(uchar4 * ptr) {
	container->Color(ptr);
}

/// Initialization of lattice nodes
/**
        Initialization of the variables in the lattice nodes
*/
void Lattice::Init()
{
   output("Initializing Lattice ...\n");
//   container->out=Snaps[0];
//   container->CopyToConst();
//   container->Init();
	iSnaps[getSnap(0)]=0;
	iSnaps[0]=0;
	Record_Iter=0;
	Iter = 0;
	container->iter = 0;
	Snap=0;
   MPI_Barrier(MPI_COMM_WORLD);
   Action_Init(1,0,ITER_NO);
   MPI_Barrier(MPI_COMM_WORLD);
	clearGlobals();
//   Stream();
//   container->Init();
//   Stream();
}

void Lattice::BeforeIt()
{
}

/// Function for calculating the index of a Snapshot for a iteration
int Lattice::getSnap(int i) {
	int s = SnapLevel(i) + 1;
	return s;
}

/// Iterate Primal till a specific iteration
/**
        Function which reconstructs a state in iteration "it"
        by reading a snapshot and iterating from it.
        Used in usteady adjoint
        \param it Iteration on which to finish
        \param iter_type Type of iterations to run
*/
void Lattice::IterateTill(int it, int iter_type)
{
	int s1,ls1,s2,ls2;
	int mx = -1, imx = -1, i;
	for (i=0;i<maxSnaps; i++) {
		if ((iSnaps[i] > mx) && (iSnaps[i] <= it)) {
			mx = iSnaps[i];
			imx = i;
		}
	}
	assert(imx >= 0);
	debug2("iterate: %d -> %d (%d primal) startSnap: %d\n", mx, it, it-mx, imx);
	if (imx >= nSnaps) {
		if (reverse_save) {
			debug2("Reverse Adjoint Disk Read it:%d level:%d\n", mx, imx);
			char filename[STRING_LEN];
			sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, imx);
			if (load(Snaps[0], filename)) exit(-1);
		} else {
			ERROR("I have to read from disk, but reverse_save is not switched on\n");
			exit (-1);
		}
		iSnaps[0] = iSnaps[imx];
		imx = 0;
	}
	s1 = imx;
	Record_Iter = mx;
	Snap = s1;
	for (i = mx; i < it; i++) {
		int s2 = getSnap(i+1);
		int s3 = s2;
		if (s2 >= nSnaps) s3=0;
		pop_settings();
		Iteration(s1, s3, iter_type);
		Record_Iter = i+1;
		if (s2 >= nSnaps){
			if (reverse_save) {
				debug2("Reverse Adjoint Disk Write it:%d level:%d\n", i+1, s2);
				char filename[STRING_LEN];
				sprintf(filename, "Snap_%02d_%02d.dat", D_MPI_RANK, s2);
				save(Snaps[0], filename);
			}
		}
		iSnaps[s2] = i+1;
		iSnaps[s3] = i+1;
		s1 = s3;
	}
}

/// Main iteration function.
/**
        Makes "niter" iterations of type "iter_type"
        Dispatches specific iteration methods for each type
        takes care of calculation of Globals
        \param niter How many iterations to make
        \param iter_type What type of iterations to make
*/
void Lattice::Iterate(int niter, int iter_type)
{
	int last_glob = iter_type & ITER_LASTGLOB;
	int i;
	InitialIteration(niter);
	if (iter_type & ITER_INTEG) last_glob=0;
	if (reverse_save) {
	        switch (iter_type & ITER_TYPE) {
	        case ITER_ADJOINT:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					container->clearGlobals();
					iter_type = iter_type | ITER_GLOBS;
				}
				Record_Iter -= 1;
				if (Record_Iter < 0) {
					ERROR("Went below the begining of the tape (usteady adjoint) !");
					exit(-1);
				}
				IterateTill(Record_Iter, ITER_NORM);
				Iteration_Adj(Snap, (Snap+1) % 2, aSnap % 2, (aSnap+1) % 2, iter_type);
			}
			break;
                case ITER_NORM:
			if (last_glob) {
				IterateTill(niter+Record_Iter-1, iter_type);
				container->clearGlobals();
				IterateTill(Record_Iter+1, iter_type | ITER_GLOBS);
			} else {
				IterateTill(niter+Record_Iter, iter_type);
			}
			break;
                case ITER_OPT:
                        ERROR("Cannot do a OPT iteration in record mode\n");
                        exit(-1);
                        break;
		}
	} else {
	        switch (iter_type & ITER_TYPE) {
	        case ITER_ADJOINT:
	                iter_type |= ITER_STEADY;
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					container->clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration_Adj(Snap, (Snap+1) % 2, aSnap, (aSnap+1) % 2, iter_type);
			}
			break;
                case ITER_NORM:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					container->clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration(Snap, (Snap+1) % 2, iter_type);
				Iter ++;
				container->iter ++;
			}
			break;
                case ITER_OPT:
			for (i=0; i< niter; i++) {
				if ((last_glob) && (i == niter -1)) {
					container->clearGlobals();
					iter_type |= ITER_GLOBS;
				}
				Iteration_Opt(Snap, (Snap+1) % 2, aSnap, (aSnap+1) % 2, iter_type);
				Iter ++;
				container->iter ++;
			}
                        break;
		}
	}
	if (last_glob) {
	        switch (iter_type & ITER_TYPE) {
                case ITER_OPT:
			clearGlobals();
	        case ITER_ADJOINT:
			clearGlobals_Adj();
			break;
                case ITER_NORM:
			clearGlobals();
			break;
		}
		calcGlobals();
	} else if ( iter_type & ITER_INTEG ) {
		calcGlobals();
	}
	FinalIteration();
};

void Lattice::AfterIt()
{
	float t;
	for (int i=0; i < 4; i++) {
		HANDLE_ERROR(CudaEventQuery(kernelEv[i]));
		CudaEventElapsedTime(&t, kernelEv[0], kernelEv[i]);
		debug2("[%d] kernelEv[%d] --> %f\n", D_MPI_RANK, i, t);
	}
	for (int i = 0; i < bufnumber; i++) {
<?R
	for (Ev in c("outEv","mpiEv","inEv")) {
?>
		HANDLE_ERROR(CudaEventQuery( <?%s Ev ?>[i] ));
		CudaEventElapsedTime(&t, kernelEv[0], <?%s Ev ?>[i]);
		debug2("[%d] <?%s Ev ?>[%d] --> %f\n", D_MPI_RANK, i, t);
<?R } ?>
	}

}

/// Overwrite NodeType in a region
void Lattice::FlagOverwrite(flag_t * mask, lbRegion over)
{
//	debug1("Overwriting flags of mesh (%lud b)\n", region.nx*region.ny*sizeof(flag_t));
//	CudaMemcpy2D(container->NodeType, sizeof(flag_t), mask, sizeof(flag_t), sizeof(flag_t), region.nx*region.ny, cudaMemcpyHostToDevice);
	lbRegion inter = region.intersect(over);
//        debug1("Overwriting flags of mesh (%ld b) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
                CudaMemcpy2D(&container->NodeType[region.offsetL(x,y,z)], sizeof(flag_t), &mask[over.offsetL(x,y,z)], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyHostToDevice);
        }

}

/// Get NodeType's from a region
void Lattice::GetFlags(lbRegion over, flag_t * NodeType)
{
//	debug1("Pulling region %dx%d+%d,%d\n",w,h,x,y);
	size_t offset;
	lbRegion inter = region.intersect(over);
//        debug1("Getting flags region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL()*sizeof(flag_t), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx;
        for (int y = inter.dy; y<inter.dy+inter.ny; y++)
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {
//		debug1("GetRegion: %d %d %d\n", x,y,z);
		offset = region.offsetL(x,y,z);
		CudaMemcpy2D(&NodeType[over.offsetL(x,y,z)], sizeof(flag_t), &container->NodeType[offset], sizeof(flag_t), sizeof(flag_t), inter.nx, cudaMemcpyDeviceToHost);
	}

}

<?R
	for (d in rows(rbind(Density,DensityAD))) { 
	for (adjoint in c(TRUE,FALSE)) {
	if (adjoint) {
		from="aSnaps[aSnap]";
		suff = "_Adj"
	} else {
		from="Snaps[Snap]";
		suff = ""
	}
	ifdef(adjoint)
?>

/// Get [<?%s d$comment ?>]
/**
        Retrive the values of the density <?%s d$nicename ?> (<?%s d$comment ?>)
        from the GPU memory
*/
void Lattice::Get_<?%s d$nicename ?><?%s suff ?>(real_t * tab)
{
	debug2("Pulling all <?%s d$nicename ?>\n");
	CudaMemcpy(
		tab,
		&<?%s from ?>.block14[<?%s d$Index ?>*region.sizeL()], 
		region.sizeL()*sizeof(real_t),
		cudaMemcpyDeviceToHost);
}

/// Clear [<?%s d$comment ?>]
/**
        Clear (set to zero) the values of
        the density <?%s d$nicename ?> (<?%s d$comment ?>)
        in the GPU memory
*/
void Lattice::Clear_<?%s d$nicename ?><?%s suff ?>()
{
	debug2("Clearing all <?%s d$nicename ?>\n");
	CudaMemset(
		&<?%s from ?>.block14[<?%s d$Index ?>*region.sizeL()], 
		0,
		region.sizeL()*sizeof(real_t));
}

/// Set [<?%s d$comment ?>]
/**
        Set the values of
        the density <?%s d$nicename ?> (<?%s d$comment ?>)
        in the GPU memory
*/
void Lattice::Set_<?%s d$nicename ?><?%s suff ?>(real_t * tab)
{
	debug2("Setting all <?%s d$nicename ?>\n");
	CudaMemcpy(
		&<?%s from ?>.block14[<?%s d$Index?>*region.sizeL()], 
		tab,
		region.sizeL()*sizeof(real_t),
		cudaMemcpyHostToDevice);
}

<?R }
ifdef()
} ?>

<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>

/// Get [<?%s q$comment ?>]
/**
        Retrive the values of the Quantity <?%s q$name ?> (<?%s q$comment ?>)
        from the GPU memory
*/
void Lattice::Get<?%s q$name ?>(lbRegion over, <?%s q$type ?> * tab, real_t scale)
{
	container->in = Snaps[Snap];
	<?R if (q$adjoint) { ?> container->adjin = aSnaps[aSnap]; <?R } ?>
        container->CopyToConst();

	lbRegion inter = region.intersect(over);
//        debug1("Getting <?%s q$name ?> of region (%ld elements) %dx%dx%d+%d,%d,%d \n", inter.sizeL(), inter.nx, inter.ny, inter.nz, inter.dx, inter.dy, inter.dz);
        int x = inter.dx, y = inter.dy;
	<?%s q$type ?> * buf=NULL;
	CudaMalloc((void**)&buf, inter.nx*inter.ny*sizeof(<?%s q$type ?>));
        for (int z = inter.dz; z<inter.dz+inter.nz; z++)
        {	lbRegion small = inter;
		small.dz = z;
		small.nz = 1;
		small.dx -= region.dx;
		small.dy -= region.dy;
		small.dz -= region.dz;
//	        debug1("Getting region (%ld elements) %dx%dx%d+%d,%d,%d \n", small.sizeL(), small.nx, small.ny, small.nz, small.dx, small.dy, small.dz);
		CudaKernelRun( get<?%s q$name ?> , dim3(small.nx,small.ny) , dim3(1) , (small, buf, scale));
//	        debug1("%d -> %d \n", over.offsetL(x,y,z), small.sizeL());
		CudaMemcpy(&tab[over.offsetL(x,y,z)], buf, small.sizeL()*sizeof(<?%s q$type ?>), cudaMemcpyDeviceToHost);
	}
	CudaFree(buf);
}

<?R }; ifdef() ?>

/// Push a setting on to the stack (recording)
/**
        Push a setting and it's value on the stack,
        during adjoint recording.
        \param i Index of the Setting
        \param old Old value of the Setting
        \param nw New value of the Setting
*/
void Lattice::push_setting(int i, real_t old, real_t nw)
{
	settings_record.push_back(
		std::pair< int, std::pair<int,std::pair<real_t,real_t> > >(
			Record_Iter,
			std::pair<int,std::pair<real_t,real_t> >(
				i,
				std::pair<real_t,real_t> (
					old,
					nw
				)
			)
		)
	);
	settings_i = settings_record.size();
}

/// Reconstruct the values of the settings in a iteration
/**
        Reconstructs the values of all the settings
        in a specific iteration of the recorded solution
        from the settin stack.
*/
void Lattice::pop_settings()
{
	while (settings_i > 0) {
		if (settings_record[settings_i - 1].first <= Record_Iter) break;
		settings_i--;
		debug1("set %d to (back) %lf -> %lf\n",settings_record[settings_i].second.first, settings_record[settings_i].second.second.second,settings_record[settings_i].second.second.first);
		setSetting(settings_record[settings_i].second.first, settings_record[settings_i].second.second.first);
	}
	while (settings_i < settings_record.size()) {
		if (settings_record[settings_i].first > Record_Iter) break;
		debug1("set %d to (front) %lf -> %lf\n",settings_record[settings_i].second.first, settings_record[settings_i].second.second.first,settings_record[settings_i].second.second.second);
		setSetting(settings_record[settings_i].second.first, settings_record[settings_i].second.second.second);
		settings_i++;
	}

}

/// Retrive the Globals
/**
        Get the Globals from GPU memory and MPI-reduce them
        \param tab Vector to store the result
*/
void Lattice::getGlobals(real_t * tab) {
        real_t tabl[ GLOBALS ];
        container->getGlobals(tabl); <?R
        by(Globals,Globals$op,function(G) { n = nrow(G); ?>
                MPI_Reduce(
                        &tabl[<?%s G$Index[1] ?>],
                        &tab[<?%s G$Index[1] ?>],
                        <?%s G$Index[n] ?> - <?%s G$Index[1] ?> + 1,
                        MPI_REAL_T,
                        MPI_<?%s G$op[1] ?>,
                        0,
                        MPI_COMM_WORLD); <?R
        }) ?>
}


/// Update the internal globals table
/**
        Retrive Globals values from GPU, reduce them and calculate the objective
*/
void Lattice::calcGlobals() {
        real_t tab[ GLOBALS ];
        getGlobals(tab);
	double obj =0;
<?R
	for (m in rows(Globals)) {
		i = which(Settings$name == paste(m$name,"InObj",sep=""));
		if (length(i) == 1) {
			s = Settings[Settings$name == paste(m$name,"InObj",sep=""),]; ?>
	obj += settings[<?%s s$Index ?>] * tab[<?%s m$Index ?>]; <?R
		}
	}
?>	
	tab[<?%s Globals$Index[Globals$name == "Objective"] ?>] += obj;
	for (int i=0; i< GLOBALS ; i++) globals[i] += tab[i];
	container->clearGlobals();
}

/// Clear the internal globals table
void Lattice::clearGlobals() { <?R
	for( g in rows(Globals) ) if (!g$adjoint) { ?>
	globals[<?%s g$Index ?>] = 0; <?R
	}
?>
}

/// Clear the internal globals table (derivative part)
void Lattice::clearGlobals_Adj() { <?R
	for( g in rows(Globals) ) if (g$adjoint) { ?>
	globals[<?%s g$Index ?>] = 0; <?R
	}
?>
}

/// Return the objective function value
double Lattice::getObjective() {
	return globals[<?%s Globals$Index[Globals$name == "Objective"] ?>];
}

/// Set a Setting
/**
        Set a specific Setting
        \param i Index of the Setting
        \param tmp Value of the Setting
*/
void Lattice::setSetting(int i, real_t tmp) {
	settings[i] = tmp;
        debug1("[%d] Settings %d to %f\n", D_MPI_RANK,i, tmp);
	setConstSetting(i,tmp);
}

<?R
        for (v in rows(Settings)) {
                if (is.na(v$derived)) {
                        tmp = paste(v$name,"_",sep="");
                } else {
                        tmp = v$name;
                }
        ?>
/// Set [<?%s v$comment ?>]
/**
        Set <?%s v$name ?> Setting (<?%s v$comment ?>). <?R
if (!is.na(v$derived)) { ?>
        <?%s v$name ?> is a dummy setting. It sets <?%s v$derived ?>
        with the equation <?%s v$derived ?> = <?%s v$equation ?><?R
} ?>
*/
void Lattice::<?%s v$FunName ?>(real_t <?%s tmp ?>) {
	if (reverse_save) push_setting(<?%s v$Index ?>, settings[<?%s v$Index ?>], <?%s tmp ?>);
        output("[%d] Settings [<?%s v$comment ?>] to %f\n", D_MPI_RANK, <?%s tmp ?>);
	setSetting(<?%s v$Index ?>, <?%s tmp ?>); <?R
                if (!is.na(v$derived)) {
                        i = which(as.character(Settings$name) == as.character(v$derived));
                        if (is.null(i)) stop(paste(v$derived, "was declared as derived of",v$name,". But", v$derived, "doesn't exist\n"));
                        sel = Settings[i[1],]; ?>
        <?%s sel$FunName ?>(<?%s v$equation ?>); <?R
                } ?>
} <?R
        } ?>

void Lattice::resetAverage() {
	container->reset_iter = container->iter;
	<?R for (d in rows(DensityAll)) if (d$average) {?> 
	  CudaMemset(&<?%s from ?>.block14[<?%s d$Index ?>*region.sizeL()],0,region.sizeL()*sizeof(real_t));	 
	<?R } ?> 
        <?R for (f in Fields)  if ((f$average) && (!(f$name %in% Density$name)))   { ?>
          CudaMemset(&<?%s from ?>.block14[<?%s f$Index ?>*region.sizeL()],0,region.sizeL()*sizeof(real_t));          
	 <?R } ?>
} 
<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>
void Lattice::GetSample<?%s q$name ?>(lbRegion over, real_t scale,real_t* buf)
{	
       	container->in = Snaps[Snap];
       	<?R if (q$adjoint) { ?> container->adjin = aSnaps[aSnap]; <?R } ?>
       	container->CopyToConst();
	lbRegion small = region.intersect(over);
	CudaKernelRun( get<?%s q$name ?> , dim3(small.nx,small.ny) , dim3(1) , (small, (<?%s q$type?>*) buf, scale));
}
<?R } ;ifdef() ?>
void Lattice::updateAllSamples(){
 if (sample->size != 0) {
	for (size_t j = 0; j < sample->spoints.size(); j++) {
		if (mpi.rank == sample->spoints[j].rank) { 
		<?R for (q in rows(Quantities)) { ifdef(q$adjoint); ?>
	 	 if (sample->quant->in("<?%s q$name ?>"))
		{
                        double v = sample->units.alt("<?%s q$unit ?>");
			GetSample<?%s q$name ?>(sample->spoints[j].location,1/v,&sample->gpu_buffer[sample->location["<?%s q$name ?>"]+(container->iter - sample->startIter)*sample->size + sample->totalIter*j*sample->size]);
		}
		<?R }; ifdef() ?>	 
	} }
 }
}


