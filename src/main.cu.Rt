<?R
        source("conf.R")
	c_header();
?>

//#include "pugixml.hpp"
#include "Global.h"
#include <mpi.h>
#ifdef GRAPHICS
//	#include "gpu_anim.h"
#endif
//#include "cross.h"
#include "Region.h"
//#include "Node.h"
//#include "LatticeContainer.h"
//#include "Lattice.h"
//#include "vtkLattice.h"
//#include "Geometry.h"
//#include "def.h"
#include "utils.h"
#include "unit.h"

#include <fstream>
#include <iostream>
#include <vector>
#include <iomanip>
#include <assert.h>

#include "Solver.h"



	void readUnits(pugi::xml_node config, Solver* solver) {
		pugi::xml_node set = config.child("Units");
		if (!set) {
			printf("[%d] Warning: No \"Units\" element in config file\n", D_MPI_RANK);
			return;
		}
                for (pugi::xml_node node = set.child("Params"); node; node = node.next_sibling("Params")) {
			std::string nm="", val="", gauge="1";
			for (pugi::xml_attribute attr = node.first_attribute(); attr; attr = attr.next_attribute())
			{
				if (((std::string) attr.name()) == "gauge") {
					gauge = attr.value();
				} else {
					if (nm != "") {
						std::cerr << "Only one variable allowed in a Params element in Units (" << nm << "," << attr.name() << ")\n";
					}
					nm = attr.name();
					val= attr.value();
				}
			}
			if (nm == "") {
				std::cerr << "No variable in a Params element in Units\n";
			}
			if (D_MPI_RANK == 0) printf("[ ] %s = %s = %s \n", nm.c_str(), val.c_str(), gauge.c_str());
			solver->setUnit(nm, val, gauge);
                }
		solver->Gauge();
	};


CudaEvent_t     start, stop; // CUDA events

int MainCallback(int iter, Solver* solver) {
	int steps;
	float   elapsedTime; // Elapsed times
	CudaEventRecord( stop, 0 );
	CudaEventSynchronize( stop );
	CudaEventElapsedTime( &elapsedTime, start, stop );
	if (D_MPI_RANK == 0) {
       		int ups = (float) (1000.*iter)/elapsedTime; // Steps made per second
       		double lbups=1.0;
       		lbups *= solver->info.region.nx;
		lbups *= solver->info.region.ny;
       		lbups *= solver->info.region.nz;
       		lbups *= iter;
		lbups /= elapsedTime;
		int desired_steps = ups/desired_fps; // Desired steps per frame (so that on next frame fps = desired_fps)
		printf("[ ] %8.1f MLBUps   %7.2f GB/s                        \r", ((double)lbups)/1000, ( (double) lbups * ((double) 2 * NUMBER_OF_DENSITIES * sizeof(real_t) + sizeof(flag_t))) / 1e6);
		fflush(stdout);
		steps = desired_steps;
		if (steps < 1) steps = 1;
		if (steps % 2 == 1) steps ++;
	}
	MPI_Bcast(&steps, 1, MPI_INT, 0, MPI_COMM_WORLD);
	solver->EventLoop();
	CudaEventRecord( start, 0 );
	CudaEventSynchronize( start );
	return steps;
}

// main program function
int main ( int argc, char * argv[] )
{
	// Error handling for scanf
	#define HANDLE_IOERR(x) if ((x) == EOF) { fprintf(stderr, "[%d] Error in fscanf.\n", D_MPI_RANK); return -1; }

	MPI_Init(&argc, &argv);

	Solver   solver(MPI_COMM_WORLD); // Global data declaration

	MPI_Comm_rank(MPI_COMM_WORLD,  &solver.mpi_rank);
	MPI_Comm_size(MPI_COMM_WORLD,  &solver.mpi_size);
	DEBUG_SETRANK(solver.mpi_rank);
	DEBUG_M;
	MPI_Barrier(MPI_COMM_WORLD);
	if (solver.mpi_rank == 0) {
		printf("[ ] -------------------------------------------------------------------------\n");
		printf("[ ] -  CLB version: %25s                               -\n",VERSION);
		printf("[ ] -        Model: %25s                               -\n",MODEL);
		printf("[ ] -------------------------------------------------------------------------\n");
	}
	MPI_Barrier(MPI_COMM_WORLD);
	DEBUG_M;

	//Prepare MPI solver-structure
	solver.mpi.node = new NodeInfo[solver.mpi_size];
	solver.mpi.size = solver.mpi_size;
	solver.mpi.rank = solver.mpi_rank;
	solver.mpi.gpu = 0;
	for (int i=0;i < solver.mpi_size; i++) solver.mpi.node[i].rank = i;


	// Reading arguments
	// At least one argument
	if ( argc < 2 ) {
		fprintf(stderr, "Usage: program configfile [device number]\n");
		return 0;
	}

	// After the configfile comes the numbers of GPU selected for each processor (starting with 0)
	{
		int count, dev;
		CudaGetDeviceCount( &count );
		if (argc >= 3) {
                	if (argc < 2 + solver.mpi.size) {
				fprintf(stderr, "Usage: program configfile [device number]\n");
				fprintf(stderr, " Provide device number for each processor (%d processors)\n", solver.mpi.size);
				return 0;
			}
			HANDLE_IOERR( sscanf(argv[2+solver.mpi.rank], "%d", &dev) );
			if (dev < 0) {
				fprintf(stderr, "Wrong device number: %s\n", argv[2+solver.mpi.rank]);
				return -1;
			}
			#ifdef GRAPHICS
				if (dev != 0) { fprintf(stderr, "Only device 0 can be selected for GUI program (not yet implemented)\n"); return -1; }
			#endif
		} else {
			CudaGetDeviceCount( &count );
			dev = solver.mpi.rank % count;
		}
		printf("[%d] Selecting device %d/%d\n", solver.mpi.rank, dev, count);
		CudaSetDevice( dev );
		solver.mpi.gpu = dev;		
	}

	MPI_Barrier(MPI_COMM_WORLD);
	DEBUG_M;

	// Calculating the right number of threads per block
	#ifdef CROSS_CPU
		solver.info.xsdim = 1;
		solver.info.ysdim = 1;
	#else
		solver.info.xsdim = 32;
		solver.info.ysdim = 1;
	#endif


	// 1024 threads not working. (Why?)
	if (solver.info.xsdim > 512) solver.info.xsdim = 512;
	printf("[%d] Running at %dx%d threads per block\n", D_MPI_RANK, solver.info.xsdim, solver.info.ysdim);
	<?R for (tp in c("size_t","real_t","vector_t","flag_t")) { ?>
		DEBUG0(printf("[%d] sizeof(<?%s tp?>) = %ld\n", D_MPI_RANK, sizeof(<?%s tp?>));)
	<?R } ?>
	MPI_Barrier(MPI_COMM_WORLD);

	// Reading the config file
	char* filename = argv[1];
	pugi::xml_document configfile;

        if (xml_def_init()) { fprintf(stderr, "[%d] Error in xml_def_init. It should work!\n", D_MPI_RANK); return -1; }
	strcpy(solver.info.conffile, filename);
	solver.setOutput("");
	pugi::xml_parse_result result = configfile.load_file(filename);
	if (!result) {
		std::cerr << "Error while parsing " << filename << ": " << result.description() << std::endl;
		return -1;
	}
	#define XMLCHILD(x,y,z) { x = y.child(z); if (!x) { std::cerr << "Error in " << filename << ": No \"" << z << "\" element" << std::endl; return -1; }}
	pugi::xml_node config, geom, units;
	XMLCHILD(config, configfile, "CLBConfig");
	XMLCHILD(geom, config, "Geometry");
	readUnits(config, &solver);

	int nx, ny, nz;
	nx = myround(solver.units.alt(geom.attribute("nx").value(),1));
	ny = myround(solver.units.alt(geom.attribute("ny").value(),1));
	nz = myround(solver.units.alt(geom.attribute("nz").value(),1));
	printf("[%d] Mesh size in config file: %dx%dx%d\n",D_MPI_RANK,nx,ny,nz);

	if (solver.setSize(nx,ny,nz)) return -1;

	CudaEventCreate( &start );
	CudaEventCreate( &stop );
	CudaEventRecord( start, 0 );
	solver.lattice->Callback((int(*)(int, void*)) MainCallback, (void*) &solver);

	Handler hand(config, &solver);
	if (!hand) {
		std::cerr << "Something went wrong in xml run!\n";
		return -1;
	}
	printf("cudaFree ...\n");
	CudaEventDestroy( start );
	CudaEventDestroy( stop );
	MPI_Finalize();
	return 0;
}


