<?R
	source("conf.R")
        c_header()
    tminx = min(0,Fields$minx)
    tmaxx = max(0,Fields$maxx)
    tminy = min(0,Fields$miny)
    tmaxy = max(0,Fields$maxy)
    tminz = min(0,Fields$minz)
    tmaxz = max(0,Fields$maxz)
?>
/** \file LatticeContainer.cu
  File defining LatticeContainer and some additional CUDA functions
*/

#include "Consts.h"
#include "Global.h"
#include "Lattice.h"
#include "mpi.h"
#define ALLOCPRINT1 debug2("Allocating: %ld b\n", size)
#define ALLOCPRINT2 debug1("got address: (%p - %p)\n", tmp, (unsigned char*)tmp+size)

int _xsdim = 0;
int _ysdim = 1;

/// Fast modulo for one k
CudaDeviceFunction inline int mymod(int x, int k){
        if (x >= k) return x-k;   
        if (x < 0) return x+k;
        return x;
}

/// Clear mem (unused)
CudaGlobalFunction void clearmem(real_t* ptr) {
	ptr[CudaBlock.x] = 0.0;
}

/// Init Settings with 0 in GPU constant memory
void initSettings() {
	real_t val = 0;
<?R for (v in rows(Settings)) {
	if (is.na(v$derived)) { ?>
			CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &val, sizeof(real_t)); <?R
	}} ?>
}

/// Set Setting in GPU constant memory
/**
  Sets a Setting in the constant memory of GPU
  \param i Index of the Setting
  \param tmp value of the Setting
*/
void setConstSetting(int i, real_t tmp) {
	switch (i) {
<?R
        for (v in rows(Settings)) if (is.na(v$derived)) { ?>
	case <?%s v$Index ?>:
	        CudaCopyToConstant("<?%s v$name ?>", <?%s v$name ?>, &tmp, sizeof(real_t));
		break; <?R
        } ?>
	}
}

/// Get maximal number of threads for all the kernels on runtime
template < class N > dim3 GetThreads() {
	dim3 ret;
	CudaFuncAttributes * attr = new CudaFuncAttributes;
	CudaFuncGetAttributes(attr, RunKernel<N>) ;
	debug1( "[%d] Constant mem:%ld\n", D_MPI_RANK, attr->constSizeBytes);
	debug1( "[%d] Local    mem:%ld\n", D_MPI_RANK, attr->localSizeBytes);
	debug1( "[%d] Max  threads:%d\n", D_MPI_RANK, attr->maxThreadsPerBlock);
	debug1( "[%d] Reg   Number:%d\n", D_MPI_RANK, attr->numRegs);
	debug1( "[%d] Shared   mem:%ld\n", D_MPI_RANK, attr->sharedSizeBytes);
	if (attr->maxThreadsPerBlock > MAX_THREADS) attr->maxThreadsPerBlock = MAX_THREADS;
	ret.x = X_BLOCK;
	ret.y = attr->maxThreadsPerBlock/ret.x;
	ret.z = 1;
	return ret;
}


template < class N > class ThreadNumber {
  public:
  static unsigned int ky, sy, maxy;
  static std::string name;
  static void Init(bool fit, size_t ny, std::string name_) {
      ny = ny - 2;
        name = name_;
	dim3 th = GetThreads< N >();
	maxy = th.y;
	sy = maxy;
	ky = ny/sy;
	if (fit) while ((sy > 1) && (ky*sy != ny)) { sy--; ky = ny/sy; }
	if (ky*th.y < ny) ky++;
	print();
  }
  static inline int getKY() { return ky; }
  static inline int getSY() { return sy; }
  static inline void print() {
    if (maxy != sy) {
	notice( "  %3dx%-3d  | %s --- Reduced from %d to fit Y dim\n", X_BLOCK, sy, name.c_str(), maxy);
    } else {
	output( "  %3dx%-3d  | %s\n", X_BLOCK, sy, name.c_str());
    }
  }
};

template < class N > unsigned int ThreadNumber< N >::ky = 0;
template < class N > unsigned int ThreadNumber< N >::sy = 0;
template < class N > unsigned int ThreadNumber< N >::maxy = 0;
template < class N > std::string ThreadNumber< N >::name = "";
  
/// Initialize Thread/Block number variables
int InitDim(int ny) {
	MPI_Barrier(MPMD.local);
        output( "  Threads  |      Action\n");
<?R	for (tp in rows(AllKernels)[order(AllKernels$adjoint)]) { ifdef(tp$adjoint) ?>
        ThreadNumber< Node_Run< <?%s tp$TemplateArgs ?> > >::Init(<?%s {if (Stages$particle[tp$Stage]) "true" else "false"} ?>, ny, "<?%s tp$TemplateArgs ?>"); <?R
	}; ifdef(); ?>
	MPI_Barrier(MPMD.local);
        return 0;
}

/// Allocation of a GPU memory Buffer
void * BAlloc(size_t size) {
  char * tmp = NULL;
    ALLOCPRINT1;
    #ifdef DIRECT_MEM
      CudaMallocHost( (void**)&tmp, size );
    #else
      CudaMalloc( (void**)&tmp, size );
    #endif
    ALLOCPRINT2;
    CudaMemset( tmp, 0, size ); 
	return (void *) tmp;
}

/// Preallocation of a buffer (combines allocation into one big allocation)
void BPreAlloc(void ** ptr, size_t size) {
    CudaMalloc( ptr, size );
}

/// Allocation of memory for an FTabs
void FTabs::Alloc(int nx,int ny,int nz) {
  size_t size;
  char * tmp = NULL;
  <?R for (m in NonEmptyMargin) { ?>
    size = (size_t) <?R C(m$Size,float=F) ?>*sizeof(real_t);
    ALLOCPRINT1;
    #ifdef DIRECT_MEM
      CudaMallocHost( (void**)&tmp, size );
    #else
      CudaMalloc( (void**)&tmp, size );
    #endif
    ALLOCPRINT2;
    CudaMemset( tmp, 0, size ); 
    <?%s m$name ?>=  (real_t*)tmp;
  <?R } ?>
}

/// Preallocation of a FTabs
/**
  Aglomerates all the allocation into one big memory chunk
*/
void FTabs::PreAlloc(int nx,int ny,int nz) {
  size_t size;
  <?R for (m in NonEmptyMargin) { ?>
    size = (size_t) <?R C(m$Size,float=F) ?>*sizeof(real_t);
    CudaPreAlloc( (void**)&<?%s m$name ?>, size );
  <?R } ?>
}

/// Clearing (zero-ing) of a FTabs
void FTabs::Clear(int nx,int ny,int nz) {
  size_t size;
  <?R for (m in NonEmptyMargin) { ?>
    size = (size_t) <?R C(m$Size,float=F) ?>*sizeof(real_t);
    CudaMemset( <?%s m$name ?>, 0, size );
  <?R } ?>
}

/// NULL-safe free
inline void MyFree(void * ptr) {
    if (ptr != NULL) {
        #ifdef DIRECT_MEM
	    CudaFreeHost( ptr );
	#else
	    CudaFree( ptr );
	#endif
	ptr = NULL;
    }
}

/// Free FTabs memory
void FTabs::Free() { <?R
    for (m in NonEmptyMargin) { ?>
    MyFree(<?%s m$name ?>); 
    <?%s m$name ?> = NULL;<?R
    } ?>
}

/// Allocation of memory of a container
void LatticeContainer::Alloc(int nx_, int ny_, int nz_)
{
    iter = 0;
    nx = nx_;
    ny = ny_;
    nz = nz_;
    kx = nx/X_BLOCK;
    ky = ny;

    InitDim(ny);

    char * tmp=NULL;
    size_t size;

    size = (size_t) nx*ny*nz*sizeof(flag_t);
	ALLOCPRINT1;
    CudaMalloc( (void**)&tmp, size );
	ALLOCPRINT2;
    CudaMemset( tmp, 0, size ); 
    NodeType = (flag_t*)tmp;

    Q = NULL;
    particle_data_size = 0;
    particle_data = NULL;

    size = (size_t) GLOBALS * sizeof(real_t);
	ALLOCPRINT1;
    CudaMalloc( (void**)&tmp, size );
	ALLOCPRINT2;
    CudaMemset( tmp, 0, size ); // CudaKernelRun(clearmem,dim3(size/sizeof(real_t)),dim3(1),((real_t*)tmp));
    Globals = (real_t*)tmp;
	ST.setsize(0, ST_GPU);
}

void LatticeContainer::ActivateCuts() {
    if (Q == NULL) {
            void * tmp;
            size_t size = (size_t) nx*ny*nz*sizeof(cut_t)*26;
                ALLOCPRINT1;
            CudaMalloc( (void**)&tmp, size );
                ALLOCPRINT2;
            CudaMemset( tmp, 0, size ); 
            Q = (cut_t*)tmp;
    }
}

/// Destroy Container
/**
  cannot do a constructor and destructor - because this class lives on GPU
*/
void LatticeContainer::Free()
{
    CudaFree( NodeType );
    if (Q != NULL) CudaFree( Q ); 
}


/// Main Kernel
/**
  iterates over all elements and runs them with RunElement function.
  constContainer.dx/dy is to calculate only internal nodes
*/
template <class N> CudaGlobalFunction void RunKernel()
{
	N now;
	now.x_ = CudaThread.x + CudaBlock.z*CudaNumberOfThreads.x + constContainer.dx;
	now.y_ = CudaThread.y + CudaBlock.x*CudaNumberOfThreads.y + constContainer.dy;
	now.z_ = CudaBlock.y+constContainer.dz;
	#ifndef GRID_3D
		for (; now.x_ < constContainer.nx; now.x_ += CudaNumberOfThreads.x) {
	#endif
		now.Pre();
		if (now.y_ < constContainer.fy) {
			now.RunElement();
		} else {
			now.OutOfDomain();
		}
		now.Glob();
	#ifndef GRID_3D
		}
	#endif
}

/// BORDER_Z defines if we need to take Z direction in consiredation border/interior
//   if a model is 2D everything in the Z direction is interior
<?R if (any(DensityAll$dz != 0)) {?>
#define BORDER_Z // The model is 3D
<?R } else { ?>
// The model is 2D (no BORDER_Z)
<?R } ?>


/// Border Kernel
/**
  iterates over border elements and runs them with RunElement function
*/
template <class N> CudaGlobalFunction void RunBorderKernel()
{
	N now;
	now.x_ = CudaThread.x + CudaBlock.y*X_BLOCK;
	now.z_ = CudaBlock.x;
	now.Pre();
<?R if (tmaxy > tminy) { ?>
	if (now.z_ < constContainer.nz) {
<?R	for (y in tminy:tmaxy) if (y > 0) { ?>
		now.y_ = <?%d y - 1 ?>;
		now.RunElement();
<?R	} else if (y < 0) { ?>
		now.y_ = constContainer.ny - <?%d -y ?>;
		now.RunElement();
<?R	} ?>
	}
<?R }
    if (tmaxz > tminz) { ?>
	now.y_ = CudaBlock.x;
	if ((now.y_ < constContainer.fy) && (now.y_ >= constContainer.dy)) {
<?R	for (z in tminz:tmaxz) if (z > 0) { ?>
		now.z_ = <?%d z - 1 ?>;
		now.RunElement();
<?R	} else if (z < 0) { ?>
		now.z_ = constContainer.nz - <?%d -z ?>;
		now.RunElement();
<?R	} ?>
	}
<?R } ?>
	now.Glob();
}


template <class N> CudaGlobalFunction void RunParticlesKernel()
{
/*	N now;
	Particle p(0.0,0.0,0.0);
	size_t i = CudaBlock.x;
	if (i < constContainer.particle_data_size) {
		p.rad = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_R];
		p.pos.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+0];
		p.pos.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+1];
		p.pos.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_POS+2];
		p.vel.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+0];
		p.vel.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+1];
		p.vel.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_VEL+2];
		p.angvel.x = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+0];
		p.angvel.y = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+1];
		p.angvel.z = constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_ANGVEL+2];
		p.force.x = 0;
		p.force.y = 0;
		p.force.z = 0;
		p.moment.x = 0;
		p.moment.y = 0;
		p.moment.z = 0;
                int pdx = floor(p.pos.x - p.rad) - constContainer.px;
                int pnx = ceil(p.pos.x + p.rad) + 1 - constContainer.px;
                if (pdx < 0) pdx = 0;
                if (pnx > constContainer.nx) pnx = constContainer.nx;
                int pdy = floor(p.pos.y - p.rad) - constContainer.py;
                int pny = ceil(p.pos.y + p.rad) + 1 - constContainer.py;
                if (pdy < 0) pdy = 0;
                if (pny > constContainer.ny) pny = constContainer.ny;
                int pdz = floor(p.pos.z - p.rad) - constContainer.pz;
                int pnz = ceil(p.pos.z + p.rad) + 1 - constContainer.pz;
                if (pdz < 0) pdz = 0;
                if (pnz > constContainer.nz) pnz = constContainer.nz;
                for (now.z_ = pdz; now.z_ < pnz; now.z_++) {	
                for (now.y_ = pdy+CudaThread.y; now.y_ < pny; now.y_+=CudaNumberOfThreads.y) {	
                for (now.x_ = pdx+CudaThread.x; now.x_ < pnx; now.x_+=CudaNumberOfThreads.x) {
                        p.diff.x = p.pos.x - now.x_ - constContainer.px;
                        p.diff.y = p.pos.y - now.y_ - constContainer.py;
                        p.diff.z = p.pos.z - now.z_ - constContainer.pz;
                        p.dist = sqrt(p.diff.x*p.diff.x + p.diff.y*p.diff.y + p.diff.z*p.diff.z);
                        p.cvel.x = p.vel.x + p.angvel.y*p.diff.z - p.angvel.z*p.diff.y;
                        p.cvel.y = p.vel.y + p.angvel.z*p.diff.x - p.angvel.x*p.diff.z;
                        p.cvel.z = p.vel.z + p.angvel.x*p.diff.y - p.angvel.y*p.diff.x;
                        now.RunElement(p);
//                    p.force.x += -p.vel.x;
//                    p.force.y += -p.vel.y;
        //		now.LoadElement();
        //		now.ExecElement(p);
        //		now.SaveElement(); 
                }
                }
                }
		real_t fx = blockSum(p.force.x);
		real_t fy = blockSum(p.force.y);
		real_t fz = blockSum(p.force.z);
		real_t afx = blockSum(p.moment.x);
		real_t afy = blockSum(p.moment.y);
		real_t afz = blockSum(p.moment.z);

//	fz = 0;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+0] = p.force.x;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+1] = p.force.y;
//		constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+2] = p.force.z;
		if (CudaThread.x == 0 && CudaThread.y == 0) {
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+0] = fx;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+1] = fy;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_FORCE+2] = fz;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+0] = afx;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+1] = afy;
			constContainer.particle_data[i*RFI_DATA_SIZE+RFI_DATA_MOMENT+2] = afz;
		}


	}
	*/
}


/// Copy oneself to the GPU constant memory
/**
  Copiers the container object to constContainer variable
  in the constant memory of the GPU
*/
void LatticeContainer::CopyToConst() {
    dx =  0;
    fx = nx;
    dy = <?%d tmaxy ?>;
    fy = ny - <?%d -tminy ?>;
    dz = <?%d tmaxz ?>;
    fz = nz - <?%d -tminz ?>;
    CudaCopyToConstant("constContainer", constContainer, this, sizeof(LatticeContainer));
}

/// Run the border kernel
/**
  Dispatch the kernel running RunElement on all border elements of the Lattice
  \param borderStream CUDA Stream to which add the kernel run
*/
template <class N> inline void LatticeContainer::RunBorderT(CudaStream_t borderStream) {
    #ifdef BORDER_Z
	    CudaKernelRunNoWait( RunBorderKernel<N> , dim3(max(ny,nz),kx,1) , dim3(X_BLOCK),(),borderStream);
    #else
	    CudaKernelRunNoWait( RunBorderKernel<N> , dim3(nz,kx,1) , dim3(X_BLOCK),(),borderStream);
    #endif
};

/// Run the interior kernel
/**
  Dispatch the kernel running RunElement on all interior elements of the lattice
  \param interiorStream CUDA Stream to which add the kernel run
*/
template <class N> inline void LatticeContainer::RunInteriorT(CudaStream_t interiorStream) {
    int nnz;
    ky = ThreadNumber< N >::getKY();
    nnz = fz - dz;
    #ifdef GRID_3D
        CudaKernelRunNoWait( RunKernel<N> , dim3(ky, nnz, kx) , dim3(X_BLOCK,ThreadNumber< N >::getSY()),(),interiorStream);
    #else
        CudaKernelRunNoWait( RunKernel<N> , dim3(ky, nnz, 1) , dim3(X_BLOCK,ThreadNumber< N >::getSY()),(),interiorStream);
    #endif
};

/// Run the particle kernel
/**
*/
template <class N> inline void LatticeContainer::RunParticlesT(CudaStream_t interiorStream) {
	if (particle_data_size > 0) {
	    CudaKernelRunNoWait( RunParticlesKernel<N> , dim3(particle_data_size,1,1) , dim3(32,8),(),interiorStream);
//	    CudaKernelRunNoWait( RunParticlesKernel<N> , dim3(particle_data_size,1,1) , dim3(1),(),interiorStream);
	}
};


template < eOperationType I, eCalculateGlobals G, eStage S >
  void LatticeContainer::RunBorder(CudaStream_t stream)   { RunBorderT< Node_Run < I, G, S > >(stream); };
template < eOperationType I, eCalculateGlobals G, eStage S >
  void LatticeContainer::RunInterior(CudaStream_t stream) { RunInteriorT< Node_Run < I, G, S > >(stream); };
template < eOperationType I, eCalculateGlobals G, eStage S >
  void LatticeContainer::RunParticles(CudaStream_t stream) { RunParticlesT< Node_Run < I, G, S > >(stream); };


  
/// Old function for graphics output
/**
  calculates the color for one node
*/
template <class N>
CudaDeviceFunction void NodeToColor( int x, int y, int z, uchar4 *optr )
{
    int offset = x+y*constContainer.nx;
    float l=0.0; float w=0.0;
    int r=0,g=0,b=0;
    N now;
    if (x < 0) return;
    if (y < 0) return;
    if (z < 0) return;
    if (x >= constContainer.nx) return;
    if (y >= constContainer.ny) return;
    if (z >= constContainer.nz) return;
    now.x_ = x;
    now.y_ = y;
    now.z_ = z;
    constContainer.getType(now);
    constContainer.pop(now);
    {
     float2 v = now.Color();
     l = v.x;
     w = v.y;
    }

if (ISFINITE(l)) {

    l = l * 111;
    if (               (l <-111)) {r = 255; g = 255; b = 255; }
    if ((l >= -111) && (l < -11)) {r = 255*(-l-11)/100; g = 255; b = 255; }
    if ((l >=  -11) && (l <  -1)) {r = 0; g = (255*(-l-1))/10; b = 255; }
    if ((l >=   -1) && (l <   0)) {r = 0; g = 0; b = 255*(-l); }
    if ((l >=    0) && (l <   1)) {r = 255*l; g = 0; b = 0; }
    if ((l >=    1) && (l <  11)) {r = 255; g = 255*(l-1)/10; b = 0; }
    if ((l >=   11) && (l < 111)) {r = 255; g = 255; b = 255*(l-11)/100; }
    if ((l >=  111)             ) {r = 255; g = 255; b = 255; }
    r=r*w;
    g=g*w + (1-w)*255;
    b=b*w;
} else {
    r=255;
    b=255;
    g=0;
}
    optr[offset].x = r;  
    optr[offset].y = g;  
    optr[offset].z = b;  
    optr[offset].w = 255;
}

/// Kernel for graphics output
CudaGlobalFunction void ColorKernel( uchar4 *optr, int z )
{
  NodeToColor< Node_Run< Primal, NoGlobals, Get > >(
    CudaThread.x+CudaBlock.x*CudaNumberOfThreads.x,
    CudaBlock.y,
    z,
    optr
  );
}

/// Runs kernel for rendering graphics
/**
  Runs the kernel for rendering graphics 
  \param optr 4-component graphics buffer
*/
void LatticeContainer::Color( uchar4 *optr ) {
   CudaCopyToConstant("constContainer", constContainer, this, sizeof(LatticeContainer));	
   CudaKernelRun( ColorKernel , dim3(kx,ny,1), dim3(X_BLOCK) ,(optr, nz/2));
};

// Functions for getting quantities
<?R
        for (q in rows(Quantities))
        {
                ifdef(q$adjoint); ?>
/// Calculate quantity [<?%s q$comment ?>] kernel
/**
  Kernel to calculate quantity <?%s q$name ?> (<?%s q$comment ?>) over a region
  \param r Lattice region to calculate the quantity
  \param tab buffor to put the calculated result
  \param scale Scale to rescale the result (for units)
*/
CudaGlobalFunction void get<?%s q$name ?>(lbRegion r, <?%s q$type ?> * tab, real_t scale)
{
	int x = CudaBlock.x+r.dx;
	int y = CudaBlock.y+r.dy; <?R
        if (q$adjoint) { ?>
          Node_Run< Adjoint, NoGlobals, Get > now; <?R
        } else { ?>
          Node_Run< Primal, NoGlobals, Get > now; <?R
        }?>
	int z;
	for (z = r.dz; z < r.dz + r.nz; z ++) {
		now.x_ = x;
		now.y_ = y;
		now.z_ = z;
		constContainer.getType(now);
		<?%s q$type ?> w;
//		if (now.NodeType) {
			constContainer.pop(now); <?R
			if (q$adjoint) { ?>
			constContainer.pop_adj(now); <?R
	                } ?>
			w = now.get<?%s q$name ?>(); <?R
			if (q$type == "vector_t") {
	                  for (coef in c("x","y","z")) { ?>
			w.<?%s coef ?> *= scale; <?R
			  }
			} else { ?>
			w *= scale; <?R
	                } ?>
//		} else { <?R
			if (q$type == "vector_t") {
	                  for (coef in c("x","y","z")) { ?>
//			w.<?%s coef ?> = nan(""); <?R
			  }
			} else { ?>
//			w = nan(""); <?R
	                } ?>
//		}
		tab[r.offset(x,y,z)] = w;
	}
}
<?R
        }
        ifdef();
?>





<?R     for (tp in rows(AllKernels)[order(AllKernels$adjoint)]) { 
		st = Stages[tp$Stage,,drop=FALSE]
		ifdef(tp$adjoint) 	
		?>
template void LatticeContainer::RunBorder < <?%s tp$TemplateArgs ?> > (CudaStream_t stream);
template void LatticeContainer::RunInterior < <?%s tp$TemplateArgs ?> > (CudaStream_t stream); <?R
         };
	ifdef();
?>
