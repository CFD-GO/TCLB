<?R
        source("conf.R")
	c_header();
?>
/*  Main program file                                          */
/*     Here we have all the initialization and the main loop   */
/*-------------------------------------------------------------*/

#include "Consts.h"
#include "Global.h"
#include <mpi.h>
#include "Region.h"
#include "utils.h"
#include "unit.h"

#include <fstream>
#include <iostream>
#include <vector>
#include <iomanip>
#include <assert.h>

#include "Solver.h"


// Reads units from configure file and applies them to the solver
void readUnits(pugi::xml_node config, Solver* solver) {
	pugi::xml_node set = config.child("Units");
	if (!set) {
		printf("[%d] Warning: No \"Units\" element in config file\n", D_MPI_RANK);
		return;
	}
	for (pugi::xml_node node = set.child("Params"); node; node = node.next_sibling("Params")) {
		std::string nm="", val="", gauge="1";
		for (pugi::xml_attribute attr = node.first_attribute(); attr; attr = attr.next_attribute())
		{
			if (((std::string) attr.name()) == "gauge") {
				gauge = attr.value();
			} else {
				if (nm != "") {
					std::cerr << "Only one variable allowed in a Params element in Units (" << nm << "," << attr.name() << ")\n";
				}
				nm = attr.name();
				val= attr.value();
			}
		}
		if (nm == "") {
			std::cerr << "No variable in a Params element in Units\n";
		}
		if (D_MPI_RANK == 0) printf("[ ] %s = %s = %s \n", nm.c_str(), val.c_str(), gauge.c_str());
		solver->setUnit(nm, val, gauge);
	}
	solver->Gauge();
};

CudaEvent_t     start, stop; // CUDA events to measure time

// Main callback function called every some iterations to display speed
int MainCallback(int iter, Solver* solver) {
	static int cum_iter =0;
	static double cum_time = 0;
	int steps;
	float   elapsedTime; // Elapsed times
	CudaEventRecord( stop, 0 );
	CudaEventSynchronize( stop );
	CudaEventElapsedTime( &elapsedTime, start, stop );
	if (D_MPI_RANK == 0) {
		int desired_steps;		
		if (iter >= 0) {
			cum_iter += iter;
			cum_time += elapsedTime;
	       		int ups = (float) (1000.*iter)/elapsedTime; // Steps made per second
       			double lbups=1.0;
       			lbups *= solver->info.region.nx;
			lbups *= solver->info.region.ny;
       			lbups *= solver->info.region.nz;
       			lbups *= iter;
			lbups /= elapsedTime;
			desired_steps = ups/desired_fps; // Desired steps per frame (so that on next frame fps = desired_fps)
			printf("[ ] %8.1f MLBUps   %7.2f GB/s                        \r", ((double)lbups)/1000, ( (double) lbups * ((double) 2 * NUMBER_OF_DENSITIES * sizeof(real_t) + sizeof(flag_t))) / 1e6);
			fflush(stdout);
		} else {
			cum_iter += - iter - 1;
			cum_time += elapsedTime;
	       		int ups = (float) (1000.*cum_iter)/cum_time; // Steps made per second
       			double lbups=1.0;
       			lbups *= solver->info.region.nx;
			lbups *= solver->info.region.ny;
       			lbups *= solver->info.region.nz;
       			lbups *= cum_iter;
			lbups /= cum_time;
			desired_steps = ups/desired_fps; // Desired steps per frame (so that on next frame fps = desired_fps)
			printf("[ ] %8.1f MLBUps   %7.2f GB/s (avg)                  \n",
				((double)lbups)/1000,
				( (double) lbups * ((double) 2 * NUMBER_OF_DENSITIES * sizeof(real_t) + sizeof(flag_t))) / 1e6);
			fflush(stdout);
			cum_iter = 0;
			cum_time = 0;
		}
		steps = desired_steps;
		if (steps < 1) steps = 1;
		if (steps % 2 == 1) steps ++;
	}
	MPI_Bcast(&steps, 1, MPI_INT, 0, MPI_COMM_WORLD);
	solver->EventLoop();
	CudaEventRecord( start, 0 );
	CudaEventSynchronize( start );
	return steps;
}

// Finds the adjoint element in config to know how many snaps to allocate
bool find_adjoint(pugi::xml_node node)
{
	if (strcmp(node.name(), "Adjoint") == 0) {
		if (strcmp(node.attribute("type").value(), "steady") == 0) {
			return false;
		} else {
			return true;
		}
	} else return false;
}

// Main program function
int main ( int argc, char * argv[] )
{
	// Error handling for scanf
	#define HANDLE_IOERR(x) if ((x) == EOF) { fprintf(stderr, "[%d] Error in fscanf.\n", D_MPI_RANK); return -1; }

	MPI_Init(&argc, &argv);

	Solver   solver(MPI_COMM_WORLD); // Global data declaration

	MPI_Comm_rank(MPI_COMM_WORLD,  &solver.mpi_rank);
	MPI_Comm_size(MPI_COMM_WORLD,  &solver.mpi_size);
	DEBUG_SETRANK(solver.mpi_rank);
	DEBUG_M;
	MPI_Barrier(MPI_COMM_WORLD);

	if (solver.mpi_rank == 0) {
		printf("[ ] -------------------------------------------------------------------------\n");
		printf("[ ] -  CLB version: %25s                               -\n",VERSION);
		printf("[ ] -        Model: %25s                               -\n",MODEL);
		printf("[ ] -------------------------------------------------------------------------\n");
	}
	MPI_Barrier(MPI_COMM_WORLD);
	DEBUG_M;

	//Prepare MPI solver-structure
	solver.mpi.node = new NodeInfo[solver.mpi_size];
	solver.mpi.size = solver.mpi_size;
	solver.mpi.rank = solver.mpi_rank;
	solver.mpi.gpu = 0;
	for (int i=0;i < solver.mpi_size; i++) solver.mpi.node[i].rank = i;

	// Reading arguments
	// At least one argument
	if ( argc < 2 ) {
		fprintf(stderr, "Usage: program configfile [device number]\n");
		return 0;
	}

	// After the configfile comes the numbers of GPU selected for each processor (starting with 0)
	{
		int count, dev;
		CudaGetDeviceCount( &count );
		if (argc >= 3) {
                	if (argc < 2 + solver.mpi.size) {
				fprintf(stderr, "Usage: program configfile [device number]\n");
				fprintf(stderr, " Provide device number for each processor (%d processors)\n", solver.mpi.size);
				return 0;
			}
			HANDLE_IOERR( sscanf(argv[2+solver.mpi.rank], "%d", &dev) );
			if (dev < 0) {
				fprintf(stderr, "Wrong device number: %s\n", argv[2+solver.mpi.rank]);
				return -1;
			}
			#ifdef GRAPHICS
				if (dev != 0) { fprintf(stderr, "Only device 0 can be selected for GUI program (not yet implemented)\n"); return -1; }
			#endif
		} else {
			CudaGetDeviceCount( &count );
			dev = solver.mpi.rank % count;
		}
		printf("[%d] Selecting device %d/%d\n", solver.mpi.rank, dev, count);
		CudaSetDevice( dev );
		solver.mpi.gpu = dev;		
	}
	MPI_Barrier(MPI_COMM_WORLD);
	DEBUG_M;

	// Calculating the right number of threads per block
	#ifdef CROSS_CPU
		solver.info.xsdim = 1;
		solver.info.ysdim = 1;
	#else
		solver.info.xsdim = 32;
		solver.info.ysdim = 1;
	#endif

	<?R for (tp in c("size_t","real_t","vector_t","flag_t")) { ?>
		DEBUG0(printf("[%d] sizeof(<?%s tp?>) = %ld\n", D_MPI_RANK, sizeof(<?%s tp?>));)
	<?R } ?>
	MPI_Barrier(MPI_COMM_WORLD);

	// Reading the config file
	char* filename = argv[1];
	pugi::xml_document configfile;
        if (xml_def_init()) { fprintf(stderr, "[%d] Error in xml_def_init. It should work!\n", D_MPI_RANK); return -1; }
	strcpy(solver.info.conffile, filename);
	solver.setOutput("");
	pugi::xml_parse_result result = configfile.load_file(filename);
	if (!result) {
		std::cerr << "Error while parsing " << filename << ": " << result.description() << std::endl;
		return -1;
	}
	#define XMLCHILD(x,y,z) { x = y.child(z); if (!x) { std::cerr << "Error in " << filename << ": No \"" << z << "\" element" << std::endl; return -1; }}
	pugi::xml_node config, geom, units;
	XMLCHILD(config, configfile, "CLBConfig");
	XMLCHILD(geom, config, "Geometry");
	readUnits(config, &solver);

	// Reading the size of mesh
	int nx, ny, nz, ns = 2;
	nx = myround(solver.units.alt(geom.attribute("nx").value(),1));
	ny = myround(solver.units.alt(geom.attribute("ny").value(),1));
	nz = myround(solver.units.alt(geom.attribute("nz").value(),1));
	printf("[%d] Mesh size in config file: %dx%dx%d\n",D_MPI_RANK,nx,ny,nz);

	// Finding the adjoint element
	pugi::xml_node adj;
	adj = configfile.find_node(find_adjoint);
	if (adj) {
		pugi::xml_attribute attr = adj.attribute("NumberOfSnaps");
		if (attr) {
			ns = attr.as_int();
		} else {
			ns = 10;
		}
		if (ns < 2) ns =2;
		printf("[%d] Will be running nonstationary adjoint at %d Snaps\n", D_MPI_RANK, ns);
	}

	// Initializing the lattice of a specific size
	if (solver.setSize(nx,ny,nz,ns)) return -1;

	// Initializing the CUDA events and setting callback
	CudaEventCreate( &start );
	CudaEventCreate( &stop );
	CudaEventRecord( start, 0 );
	solver.lattice->Callback((int(*)(int, void*)) MainCallback, (void*) &solver);

	// Running main handler (it makes all the magic)
	Handler hand(config, &solver);
	if (!hand) {
		std::cerr << "Something went wrong in xml run!\n";
		return -1;
	}
	
	// Finish and clean up
	printf("cudaFree ...\n");
	CudaEventDestroy( start );
	CudaEventDestroy( stop );
	MPI_Finalize();
	return 0;
}


